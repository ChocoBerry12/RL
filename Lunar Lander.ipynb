{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander (discrete mode)\n",
    "## A2C\n",
    "---\n",
    "* state : (pos.x - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2),\n",
    "            (pos.y - (self.helipad_y+LEG_DOWN/SCALE)) / (VIEWPORT_H/SCALE/2),\n",
    "            vel.x*(VIEWPORT_W/SCALE/2)/FPS,\n",
    "            vel.y*(VIEWPORT_H/SCALE/2)/FPS,\n",
    "            self.lander.angle,\n",
    "            20.0*self.lander.angularVelocity/FPS,\n",
    "            1.0 if self.legs[0].ground_contact else 0.0,\n",
    "            1.0 if self.legs[1].ground_contact else 0.0\n",
    "            ]\n",
    "* action : No, 우측 엔진, 메인 엔진, 좌측 엔진\n",
    "* reward : 메인 엔진 : -0.3, 측면 엔진 : -0.03, 다리 하나 착지에 + 10, 충돌하면 -100, 목표에 착지하면 +100, 착륙선 상태에 대한 보상\n",
    "* terminal : 다리가 아닌 몸체가 충돌하거나 착지하면 종료, 200 점 달성이 목표\n",
    "* 연료는 무한\n",
    "* 최대한 정자세로 최소한의 연료로 착지하는 방향으로 학습\n",
    "* 눈으로 확인해서 적당히 되면 학습된거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -114 57\n",
      "3 -210 115\n",
      "4 -192 114\n",
      "5 -259 128\n",
      "6 -224 97\n",
      "7 -119 130\n",
      "8 -46 91\n",
      "9 -263 118\n",
      "10 -76 69\n",
      "11 -350 77\n",
      "12 -273 84\n",
      "13 -95 86\n",
      "14 -154 118\n",
      "15 -318 112\n",
      "16 -299 101\n",
      "17 -20 110\n",
      "18 -29 96\n",
      "19 -58 83\n",
      "20 -215 113\n",
      "21 -199 126\n",
      "22 -123 108\n",
      "23 -381 87\n",
      "24 -68 75\n",
      "25 -416 120\n",
      "26 -197 120\n",
      "27 -371 120\n",
      "28 -107 98\n",
      "29 -223 98\n",
      "30 -103 84\n",
      "31 -340 74\n",
      "32 -240 80\n",
      "33 -30 69\n",
      "34 -149 138\n",
      "35 -67 67\n",
      "36 -147 72\n",
      "37 -56 105\n",
      "38 -81 90\n",
      "39 -289 95\n",
      "40 -126 98\n",
      "41 -237 116\n",
      "42 -94 96\n",
      "43 -168 72\n",
      "44 -421 87\n",
      "45 -250 84\n",
      "46 -122 132\n",
      "47 -382 134\n",
      "48 -102 95\n",
      "49 -94 72\n",
      "50 -131 87\n",
      "51 -105 75\n",
      "52 -139 111\n",
      "53 -268 112\n",
      "54 -170 73\n",
      "55 -120 88\n",
      "56 -398 95\n",
      "57 -252 107\n",
      "58 -74 107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-03857d38cd48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-03857d38cd48>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(critic, critic_t, data, optim_a, optim_v)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mlog_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml_p_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms2_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlog_p\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0madvantage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0moptim_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0moptim_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-03857d38cd48>\u001b[0m in \u001b[0;36mv\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# value 는 advantage 계산을 위한 것이므로 0차원 스칼라\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1407\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import random\n",
    "import collections\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# hyper parameters\n",
    "EPSILON = 1\n",
    "ALPHA = .001\n",
    "GAMMA = .95\n",
    "\n",
    "# actor\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc_1 = nn.Linear(8, 32)\n",
    "        self.fc_pi = nn.Linear(32, 4)\n",
    "    \n",
    "    # policy 는 discrete action 의 확률이므로 softmax 통과\n",
    "    def pi(self, x):\n",
    "        x = torch.relu(self.fc_1(x))\n",
    "        x = torch.softmax(self.fc_pi(x), dim=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "# critic\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "            super(Critic, self).__init__()\n",
    "            self.fc_1 = nn.Linear(8, 32)\n",
    "            self.fc_v = nn.Linear(32, 1)\n",
    "    \n",
    "    # value 는 advantage 계산을 위한 것이므로 0차원 스칼라\n",
    "    def v(self, x):\n",
    "        x = torch.relu(self.fc_1(x))\n",
    "        x = self.fc_v(x)\n",
    "        return x\n",
    "\n",
    "            \n",
    "def train(critic, critic_t, data, optim_a, optim_v):\n",
    "    batch = random.sample(data, 64)\n",
    "    s_, a_, r_, s2_, l_p_ = [], [], [], [], []\n",
    "    for tr in batch:\n",
    "        s, a, r, s2, l_p = tr\n",
    "        s_.append(s)\n",
    "        a_.append([a])\n",
    "        r_.append([r])\n",
    "        s2_.append(s2)\n",
    "        l_p_.append([l_p])\n",
    "    s_ = torch.tensor(s_, dtype=torch.float)\n",
    "    a_ = torch.tensor(a_, dtype=torch.float)\n",
    "    r_ = torch.tensor(r_, dtype=torch.float)\n",
    "    s2_ = torch.tensor(s2_, dtype=torch.float)\n",
    "    l_p_ = torch.tensor(l_p_, dtype=torch.float)\n",
    "    \n",
    "    v_target = r_ + GAMMA * critic_t.v(s2_)\n",
    "    advantage = v_target - critic.v(s_)\n",
    "     \n",
    "    log_p = l_p_\n",
    "    loss = F.mse_loss(v_target.detach(), critic.v(s2_) -log_p * advantage.detach())\n",
    "    optim_a.zero_grad()\n",
    "    optim_v.zero_grad()\n",
    "    loss.mean().backward()\n",
    "    optim_a.step()\n",
    "    optim_v.step()\n",
    "    \n",
    "ep = 1\n",
    "total_ep = 1000\n",
    "actor, actor_t, critic, critic_t = Actor(), Actor(), Critic(), Critic()\n",
    "optimizer_a = optim.Adam(actor.parameters(), ALPHA)\n",
    "optimizer_c = optim.Adam(critic.parameters(), ALPHA)\n",
    "critic_t.load_state_dict(critic.state_dict())\n",
    "buffer = collections.deque(maxlen = 50000)\n",
    "\n",
    "while(ep < total_ep):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    while(not done):\n",
    "        if(ep > 400):\n",
    "            env.render()\n",
    "        prob = actor.pi(torch.from_numpy(state).float())\n",
    "        m = Categorical(prob)\n",
    "        \n",
    "        if(random.random() < EPSILON):\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = m.sample().item()\n",
    "        \n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        #print(reward)\n",
    "        log_prob = torch.log(prob[action])\n",
    "        transition = (state, action, reward, state_next, log_prob)\n",
    "        buffer.append(transition)\n",
    "        if(len(buffer) > 2000):\n",
    "            train(critic, critic_t, buffer, optimizer_a, optimizer_c)\n",
    "        state = state_next\n",
    "        step += 1\n",
    "        \n",
    "        if(done):\n",
    "            ep += 1\n",
    "            EPSILON = 1 / ((ep / 100) + 2)\n",
    "            print(ep, int(total_reward), step)\n",
    "            if(ep % 10 == 0):\n",
    "                critic_t.load_state_dict(critic.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "* https://www.kaggle.com/thimac/ppo-lunar-lander-reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogSoftmax' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-8b9c30e92c67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    537\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 539\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogSoftmax' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# hyper parameters\n",
    "ALPHA = .001\n",
    "EPSILON = 1\n",
    "T = 10 # T step 만큼 데이터 쌓고 학습할 것\n",
    "LAMBDA = .95\n",
    "K = 5\n",
    "GAMMA = .99\n",
    "e = .2\n",
    "\n",
    "class PPO_p(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO_p, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc_pi = nn.Linear(256, 4)\n",
    "        self.optimizer = optim.Adam(self.parameters(), ALPHA)\n",
    "    \n",
    "    def pi(self, x, softmax_dim=0):\n",
    "        x = torch.selu(self.fc1(x))\n",
    "        x = torch.selu(self.fc2(x))\n",
    "        prob = nn.LogSoftmax(self.fc_pi(x)) # batch 처리 (학습할 떈 1 차원)\n",
    "        return prob\n",
    "\n",
    "\n",
    "class PPO_v(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO_v, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc_v = nn.Linear(256, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), ALPHA)\n",
    "\n",
    "    def v(self, x):\n",
    "        x = torch.selu(self.fc1(x))\n",
    "        x = torch.selu(self.fc2(x))\n",
    "        x = self.fc_v(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_net(pi, v, data, optimizer_p, optimizer_v):\n",
    "    s, a, r, s2, d, prob = batch_factory(data)\n",
    "\n",
    "    # epoch K 만큼\n",
    "    for i in range(K):\n",
    "        td_target = r + GAMMA * net.v(s2)\n",
    "        delta = td_target - net.v(s)\n",
    "        delta = delta.detach().numpy() # 1 step advantage\n",
    "        advantage_lst = []\n",
    "        advantage = 0.0\n",
    "\n",
    "        # GAE 계산\n",
    "        for delta_t in delta[::-1]:\n",
    "            advantage = GAMMA * LAMBDA * advantage + delta_t[0]\n",
    "            advantage_lst.append([advantage])\n",
    "        advantage_lst.reverse()\n",
    "        advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "        p = pi.pi(s, softmax_dim=1)\n",
    "        p_a = p.gather(1, a)\n",
    "        ratio = torch.exp(torch.log(p_a) - torch.log(prob))\n",
    "\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - e, 1 + e) * advantage\n",
    "        loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(v.v(s) , td_target.detach())\n",
    "\n",
    "        optimizer_p.zero_grad()\n",
    "        optimizer_v.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        optimizer_p.step()\n",
    "        optimizer_v.step()\n",
    "\n",
    "def batch_factory(memory):\n",
    "    s_, a_, r_, s2_, d_, prob_ = [], [], [], [], [], []\n",
    "    for s, a, r, s2, d, p in memory:\n",
    "        s_.append(s)\n",
    "        a_.append([a])\n",
    "        r = -100 if d else r\n",
    "        r_.append([r])\n",
    "        s2_.append(s2)\n",
    "        d = 0 if d else 1\n",
    "        d_.append([d])\n",
    "        prob_.append([p])\n",
    "        \n",
    "    s_ = torch.tensor(s_, dtype=torch.float)\n",
    "    a_ = torch.tensor(a_)\n",
    "    r_ = torch.tensor(r_, dtype=torch.float)\n",
    "    s2_ = torch.tensor(s2_, dtype=torch.float)\n",
    "    d_ = torch.tensor(d_, dtype=torch.float)\n",
    "    prob_ = torch.tensor(prob_)\n",
    "    \n",
    "    return s_, a_, r_, s2_, d_, prob_\n",
    "\n",
    "pi = PPO_p()\n",
    "v = PPO_v()\n",
    "ep = 1\n",
    "total_ep = 10000\n",
    "gamma = .95\n",
    "total_reward = 0\n",
    "data = []\n",
    "optimizer_p = optim.Adam(pi.parameters(), ALPHA)\n",
    "optimizer_v = optim.Adam(v.parameters(), ALPHA)\n",
    "\n",
    "while(ep < total_ep):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    \n",
    "    while(not done):\n",
    "        # T step 움직인 후 clipping - T 가 너무 크면 불안정??\n",
    "        for t in range(T):\n",
    "            if(ep > 1000):\n",
    "                env.render()\n",
    "            step += 1\n",
    "            prob = pi.pi(torch.from_numpy(state).float())\n",
    "            prob = Categorical(prob.squeeze(0).cpu())\n",
    "            if(random.random() < EPSILON):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = Categorical(prob).sample().cpu()\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            data.append((state, action, reward/100.0, state_next, done, prob[action].item()))\n",
    "            state = state_next\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        train_net(pi, v, data, optimizer_p, optimizer_v)\n",
    "        data = []\n",
    "        \n",
    "    ep += 1\n",
    "    EPSILON = 1 / (ep/50 + 2)\n",
    "    if(ep%1 == 0):\n",
    "        print(ep, int(total_reward), step)\n",
    "        total_reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "EPSILON = 1\n",
    "EPISODE = 2000\n",
    "GAMMA = .98\n",
    "ALPHA = .001\n",
    "Q_TARG_PERIOD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        \n",
    "    def Q(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        #x = F.softmax(self.fc3(x), dim=0)\n",
    "        #x = F.softmax(self.fc3(x), dim=0)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_tar, optimizer):\n",
    "    \n",
    "    # buffer 에서 랜덤으로 데이터 뽑기\n",
    "    batch = random.sample(buffer, 32)\n",
    "    s_buf, a_buf, r_buf, s2_buf, d_buf = [], [], [], [], []\n",
    "    \n",
    "    # 학습리스트에 데이터 할당\n",
    "    for transition in batch:\n",
    "        s, a, r, s2, d = transition\n",
    "        s_buf.append(s)\n",
    "        a_buf.append([a])\n",
    "        r_buf.append([r])\n",
    "        s2_buf.append(s2)\n",
    "        d_buf.append([d])\n",
    "    \n",
    "    s_buf = torch.tensor(s_buf, dtype=torch.float)\n",
    "    a_buf = torch.tensor(a_buf)\n",
    "    r_buf = torch.tensor(r_buf)\n",
    "    s2_buf = torch.tensor(s2_buf, dtype=torch.float)\n",
    "    d_buf = torch.tensor(d_buf)\n",
    "\n",
    "    # Q 계산\n",
    "    Q = q.Q(s_buf)\n",
    "    Q = Q.gather(1, a_buf) # a_buf 를 index 로 취급하여 Q 의 값을 추려낸다.\n",
    "    \n",
    "    # target Q 계산\n",
    "    max_Q = q_tar.Q(s2_buf).max(1)[0].unsqueeze(1) # 차원 줄이거나 늘리기. view 함수도 차원변환함\n",
    "    Q_targ = r_buf + GAMMA * max_Q\n",
    "    \n",
    "    # double DQN 업데이트\n",
    "    #a = q_net.Q(s2_buf).max(1)[0].unsqueeze(1)\n",
    "    #double_q = q_tar.Q(s2_buf).gather(1, a_buf)\n",
    "    #Q_targ = r_buf + GAMMA * double_q\n",
    "    \n",
    "    # loss\n",
    "    # mse 대신 huber loss 사용 - 덜 민감해서 급격한 변화 방지\n",
    "    # It is less sensitive to outliers than the MSELoss and in some cases prevents exploding gradients \n",
    "    loss = F.smooth_l1_loss(Q, Q_targ)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -139 104\n",
      "2 -92 73\n",
      "3 -114 139\n",
      "4 -305 99\n",
      "5 -135 62\n",
      "6 -41 139\n",
      "7 -85 89\n",
      "8 -377 101\n",
      "9 -251 104\n",
      "10 -447 101\n",
      "11 -201 141\n",
      "12 -146 102\n",
      "13 -108 121\n",
      "14 -157 66\n",
      "15 -255 113\n",
      "16 -177 109\n",
      "17 -82 68\n",
      "18 -303 103\n",
      "19 -169 86\n",
      "20 -260 145\n",
      "21 -451 125\n",
      "22 -470 86\n",
      "23 -100 124\n",
      "24 -106 94\n",
      "25 -318 163\n",
      "26 -24 101\n",
      "27 -40 110\n",
      "28 -284 110\n",
      "29 -114 117\n",
      "30 -343 105\n",
      "31 -105 165\n",
      "32 -56 85\n",
      "33 -435 168\n",
      "34 -155 127\n",
      "35 -138 70\n",
      "36 -491 122\n",
      "37 -175 126\n",
      "38 -322 86\n",
      "39 -130 180\n",
      "40 -165 129\n",
      "41 -143 96\n",
      "42 -47 105\n",
      "43 -85 147\n",
      "44 -124 78\n",
      "45 -82 96\n",
      "46 -106 113\n",
      "47 -83 159\n",
      "48 -6 168\n",
      "49 -105 185\n",
      "50 -200 106\n",
      "51 -108 145\n",
      "52 -95 114\n",
      "53 -92 137\n",
      "54 -147 106\n",
      "55 -148 165\n",
      "56 -148 111\n",
      "57 -189 114\n",
      "58 -18 158\n",
      "59 -119 82\n",
      "60 -221 161\n",
      "61 -123 120\n",
      "62 -63 187\n",
      "63 -102 117\n",
      "64 -82 110\n",
      "65 -106 123\n",
      "66 -143 176\n",
      "67 -114 149\n",
      "68 -124 174\n",
      "69 -197 143\n",
      "70 -44 92\n",
      "71 -95 82\n",
      "72 -125 125\n",
      "73 37 80\n",
      "74 -103 170\n",
      "75 -76 105\n",
      "76 -80 92\n",
      "77 -40 1000\n",
      "78 -15 205\n",
      "79 -195 178\n",
      "80 0 133\n",
      "81 -121 141\n",
      "82 -67 98\n",
      "83 -72 129\n",
      "84 -122 116\n",
      "85 -53 153\n",
      "86 -29 1000\n",
      "87 -17 264\n",
      "88 -257 216\n",
      "89 -187 247\n",
      "90 -25 256\n",
      "91 -57 134\n",
      "92 -80 356\n",
      "93 -9 200\n",
      "94 -88 174\n",
      "95 -12 187\n",
      "96 -76 182\n",
      "97 -54 178\n",
      "98 -42 395\n",
      "99 -89 285\n",
      "100 22 153\n",
      "101 -45 125\n",
      "102 -38 182\n",
      "103 -122 112\n",
      "104 -11 137\n",
      "105 -41 199\n",
      "106 -23 117\n",
      "107 -17 377\n",
      "108 -89 124\n",
      "109 -57 162\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-aa89a4824c02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Q value 뽑기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mgeom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender1\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[0mglVertex3f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# draw each vertex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mglEnd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_circle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck_glend\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0merrcheck_glend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "buffer = collections.deque(maxlen = 50000)\n",
    "ep = 1\n",
    "\n",
    "# network 생성\n",
    "q_net = DQN_Net()\n",
    "q_targ = DQN_Net()\n",
    "\n",
    "# target net 에 train net\n",
    "q_targ.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# env 초기화\n",
    "state = env.reset()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(q_net.parameters(), ALPHA)\n",
    "\n",
    "while(ep < EPISODE):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    #EPSILON = max(0.01, 0.08 - 0.01*(ep/200))\n",
    "    \n",
    "    while(not done):\n",
    "        if(ep > 100):\n",
    "            env.render()\n",
    "            \n",
    "        # Q value 뽑기\n",
    "        Q_value = q_net.Q(torch.from_numpy(state).float())\n",
    "\n",
    "        # action 선택\n",
    "        if(random.random() < EPSILON):\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = Q_value.argmax().item()\n",
    "\n",
    "        # step 진행\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # reward 합산\n",
    "        total_reward += reward\n",
    "        \n",
    "        # buffer 에 data stack\n",
    "        buffer.append((state, action, reward, state_next, done))\n",
    "        \n",
    "        # state 갱신\n",
    "        state = state_next\n",
    "        step += 1\n",
    "        \n",
    "        # 학습\n",
    "        if(len(buffer) > 2000):\n",
    "            train(q_net, q_targ, optimizer)\n",
    "        \n",
    "        if(done):\n",
    "            \n",
    "            # periodical Update target net\n",
    "            if ep % Q_TARG_PERIOD == 0:\n",
    "                q_targ.load_state_dict(q_net.state_dict())\n",
    "                \n",
    "            print(ep, int(total_reward), step)\n",
    "            ep += 1\n",
    "            EPSILON = 1 / ((ep / 100) + 1)\n",
    "            state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIONS=4\n",
      "Press keys 1 2 3 ... to take actions 1 2 3 ...\n",
      "No keys pressed is taking action 0\n",
      "reward -1.862\n",
      "reward -1.912\n",
      "reward -1.856\n",
      "reward -1.799\n",
      "reward -1.742\n",
      "reward -1.685\n",
      "reward -1.627\n",
      "reward -1.569\n",
      "reward -1.511\n",
      "reward -1.452\n",
      "reward -1.394\n",
      "reward -1.335\n",
      "reward -1.276\n",
      "reward -1.217\n",
      "reward -1.157\n",
      "reward -1.098\n",
      "reward -1.039\n",
      "reward -0.979\n",
      "reward -0.920\n",
      "reward -0.860\n",
      "reward -0.801\n",
      "reward -0.741\n",
      "reward -0.681\n",
      "reward -0.622\n",
      "reward -0.655\n",
      "reward -0.500\n",
      "reward -0.440\n",
      "reward -0.380\n",
      "reward -0.321\n",
      "reward -0.261\n",
      "reward -0.201\n",
      "reward -0.142\n",
      "reward -0.082\n",
      "reward -0.022\n",
      "reward 0.037\n",
      "reward 0.097\n",
      "reward 0.157\n",
      "reward 0.216\n",
      "reward 0.276\n",
      "reward 0.335\n",
      "reward 0.395\n",
      "reward 0.454\n",
      "reward 0.514\n",
      "reward 0.573\n",
      "reward 0.632\n",
      "reward 0.691\n",
      "reward 0.749\n",
      "reward 0.807\n",
      "reward 0.864\n",
      "reward 0.921\n",
      "reward 0.975\n",
      "reward 1.027\n",
      "reward 1.075\n",
      "reward 1.112\n",
      "reward 1.127\n",
      "reward 1.075\n",
      "reward 0.728\n",
      "reward 8.593\n",
      "reward -100.000\n",
      "timesteps 59 reward -110.71\n",
      "reward -0.509\n",
      "reward -0.591\n",
      "reward -0.609\n",
      "reward -0.631\n",
      "reward -0.656\n",
      "reward -0.682\n",
      "reward -0.711\n",
      "reward -0.741\n",
      "reward -0.773\n",
      "reward -0.805\n",
      "reward -0.839\n",
      "reward -0.874\n",
      "reward -0.908\n",
      "reward -0.943\n",
      "reward -0.978\n",
      "reward -1.012\n",
      "reward -1.045\n",
      "reward -1.077\n",
      "reward -1.108\n",
      "reward -1.137\n",
      "reward -1.164\n",
      "reward -1.190\n",
      "reward -1.213\n",
      "reward -1.234\n",
      "reward -1.273\n",
      "reward -1.295\n",
      "reward -1.282\n",
      "reward -1.294\n",
      "reward -1.303\n",
      "reward -1.311\n",
      "reward -1.316\n",
      "reward -1.318\n",
      "reward -1.319\n",
      "timesteps 33 reward -33.14\n",
      "reward -0.240\n",
      "reward -0.314\n",
      "reward -0.346\n",
      "reward -0.384\n",
      "timesteps 4 reward -1.28\n",
      "reward -0.155\n",
      "reward -0.309\n",
      "reward -0.425\n",
      "reward -0.542\n",
      "reward -0.657\n",
      "reward -0.767\n",
      "reward -0.872\n",
      "reward -0.968\n",
      "reward -1.055\n",
      "reward -1.133\n",
      "reward -1.202\n",
      "reward -1.260\n",
      "reward -1.308\n",
      "reward -1.347\n",
      "reward -1.378\n",
      "reward -1.400\n",
      "reward -1.415\n",
      "reward -1.423\n",
      "reward -1.424\n",
      "reward -1.421\n",
      "reward -1.412\n",
      "reward -1.398\n",
      "reward -1.381\n",
      "reward -1.360\n",
      "reward -1.403\n",
      "reward -1.307\n",
      "reward -1.277\n",
      "reward -1.245\n",
      "reward -1.212\n",
      "reward -1.176\n",
      "reward -1.139\n",
      "reward -1.100\n",
      "timesteps 32 reward -35.87\n",
      "reward -1.167\n",
      "reward -1.280\n",
      "reward -1.262\n",
      "reward -1.244\n",
      "reward -1.222\n",
      "reward -1.198\n",
      "timesteps 6 reward -7.37\n",
      "reward -0.662\n",
      "reward -0.808\n",
      "reward -0.863\n",
      "reward -0.916\n",
      "timesteps 4 reward -3.25\n",
      "reward 0.355\n",
      "reward 0.315\n",
      "timesteps 2 reward 0.67\n",
      "reward -1.105\n",
      "reward -1.266\n",
      "reward -1.324\n",
      "reward -1.371\n",
      "reward -1.407\n",
      "reward -1.433\n",
      "reward -1.451\n",
      "reward -1.460\n",
      "reward -1.463\n",
      "reward -1.459\n",
      "reward -1.449\n",
      "reward -1.435\n",
      "reward -1.416\n",
      "reward -1.393\n",
      "reward -1.367\n",
      "reward -1.338\n",
      "reward -1.306\n",
      "reward -1.271\n",
      "reward -1.235\n",
      "reward -1.197\n",
      "reward -1.157\n",
      "reward -1.115\n",
      "reward -1.073\n",
      "reward -1.029\n",
      "reward -1.064\n",
      "reward -0.937\n",
      "reward -0.891\n",
      "reward -0.844\n",
      "reward -0.797\n",
      "reward -0.749\n",
      "reward -0.701\n",
      "reward -0.653\n",
      "reward -0.024\n",
      "reward 0.031\n",
      "reward 0.470\n",
      "reward 0.659\n",
      "reward 0.903\n",
      "reward 1.074\n",
      "reward 4.955\n",
      "reward 0.382\n",
      "reward 0.050\n",
      "reward 0.045\n",
      "reward -0.267\n",
      "reward -0.299\n",
      "reward 0.033\n",
      "reward 0.068\n",
      "reward 0.533\n",
      "reward 0.313\n",
      "reward 0.334\n",
      "reward 0.348\n",
      "reward 0.353\n",
      "reward 0.345\n",
      "reward 0.320\n",
      "reward 0.274\n",
      "reward 0.198\n",
      "reward 0.083\n",
      "reward -0.087\n",
      "reward -0.330\n",
      "reward -0.667\n",
      "reward -1.117\n",
      "reward -1.691\n",
      "reward -2.369\n",
      "reward 16.898\n",
      "reward -100.000\n",
      "timesteps 64 reward -116.34\n",
      "reward -1.433\n",
      "reward -1.566\n",
      "reward -1.589\n",
      "reward -1.602\n",
      "reward -1.604\n",
      "reward -1.599\n",
      "reward -1.068\n",
      "reward 4.636\n",
      "reward 3.807\n",
      "reward 0.777\n",
      "reward 1.217\n",
      "reward -1.659\n",
      "reward -1.654\n",
      "reward -1.642\n",
      "reward -1.624\n",
      "reward -1.600\n",
      "reward -1.571\n",
      "reward -1.539\n",
      "reward -1.503\n",
      "reward -1.464\n",
      "reward -1.423\n",
      "reward -1.380\n",
      "reward -1.336\n",
      "reward -1.290\n",
      "reward -1.999\n",
      "reward -1.991\n",
      "reward -1.540\n",
      "reward -2.096\n",
      "reward -2.390\n",
      "reward -1.810\n",
      "reward -1.760\n",
      "reward -1.710\n",
      "reward -1.660\n",
      "reward -1.608\n",
      "reward -1.557\n",
      "reward -1.506\n",
      "reward -1.454\n",
      "reward -1.402\n",
      "reward -1.350\n",
      "reward -1.298\n",
      "reward -1.247\n",
      "reward -1.750\n",
      "reward -1.897\n",
      "reward -1.456\n",
      "reward 1.778\n",
      "reward -1.502\n",
      "reward 1.982\n",
      "reward 1.352\n",
      "reward -0.917\n",
      "reward -0.790\n",
      "reward -0.339\n",
      "reward -0.828\n",
      "reward -0.794\n",
      "reward -0.764\n",
      "reward -0.740\n",
      "reward -0.724\n",
      "reward -0.716\n",
      "reward -0.722\n",
      "reward -0.746\n",
      "reward -0.793\n",
      "reward -0.874\n",
      "reward -1.002\n",
      "reward -1.196\n",
      "reward -1.480\n",
      "reward -1.883\n",
      "reward -2.426\n",
      "reward 6.895\n",
      "reward -100.000\n",
      "timesteps 68 reward -160.42\n",
      "reward -0.996\n",
      "reward -0.276\n",
      "reward 0.023\n",
      "reward -0.876\n",
      "reward 0.062\n",
      "reward -0.748\n",
      "reward 0.436\n",
      "reward 0.197\n",
      "reward 0.633\n",
      "reward 0.503\n",
      "reward 0.165\n",
      "reward 0.025\n",
      "reward -2.264\n",
      "reward -2.135\n",
      "reward -1.866\n",
      "reward -1.467\n",
      "reward -1.174\n",
      "reward -1.076\n",
      "reward -0.793\n",
      "reward -0.484\n",
      "reward -0.359\n",
      "reward 0.052\n",
      "reward -2.275\n",
      "reward -2.417\n",
      "reward -2.759\n",
      "reward -2.789\n",
      "reward -3.049\n",
      "reward -3.068\n",
      "reward -3.404\n",
      "reward -3.436\n",
      "reward -3.687\n",
      "reward -3.923\n",
      "reward -4.099\n",
      "reward -3.320\n",
      "reward -3.294\n",
      "reward -3.268\n",
      "reward -3.243\n",
      "reward -3.220\n",
      "reward -3.199\n",
      "reward -3.181\n",
      "reward -3.165\n",
      "reward -0.456\n",
      "reward -2.968\n",
      "reward -2.430\n",
      "reward -3.497\n",
      "reward -2.679\n",
      "reward -2.426\n",
      "reward -5.184\n",
      "reward -2.588\n",
      "reward -2.582\n",
      "reward -2.166\n",
      "reward -2.356\n",
      "reward -2.152\n",
      "reward -2.763\n",
      "reward -4.242\n",
      "reward -4.748\n",
      "reward -4.966\n",
      "reward -5.331\n",
      "reward -4.803\n",
      "reward -5.033\n",
      "reward 61.748\n",
      "reward -14.222\n",
      "reward -14.392\n",
      "reward -14.287\n",
      "reward -14.073\n",
      "reward -14.108\n",
      "reward -24.151\n",
      "reward -100.000\n",
      "timesteps 68 reward -270.07\n",
      "reward -0.169\n",
      "reward -0.337\n",
      "reward -0.469\n",
      "reward -0.598\n",
      "reward -0.724\n",
      "reward -0.842\n",
      "reward -0.951\n",
      "reward -1.050\n",
      "reward -1.138\n",
      "reward -1.215\n",
      "reward -1.279\n",
      "reward -1.333\n",
      "reward -1.376\n",
      "reward -1.409\n",
      "reward -1.433\n",
      "reward -1.448\n",
      "reward -1.456\n",
      "reward -1.457\n",
      "reward -1.453\n",
      "reward -1.442\n",
      "reward -1.428\n",
      "reward -1.409\n",
      "reward -1.386\n",
      "reward -1.360\n",
      "reward -1.402\n",
      "reward -1.298\n",
      "reward -1.264\n",
      "reward -1.228\n",
      "reward -1.191\n",
      "reward -1.152\n",
      "reward -1.111\n",
      "reward -1.783\n",
      "reward -1.818\n",
      "reward -2.145\n",
      "reward -2.220\n",
      "reward -1.641\n",
      "reward -1.598\n",
      "reward -1.555\n",
      "reward 1.397\n",
      "reward 2.352\n",
      "reward 1.444\n",
      "reward 0.906\n",
      "reward 0.873\n",
      "reward -1.726\n",
      "reward -0.984\n",
      "reward -0.889\n",
      "reward -0.667\n",
      "reward -0.305\n",
      "reward 0.031\n",
      "reward 0.273\n",
      "reward -0.304\n",
      "reward -0.268\n",
      "reward -0.235\n",
      "reward -0.203\n",
      "reward -0.174\n",
      "reward -0.149\n",
      "reward -0.127\n",
      "reward -0.110\n",
      "reward -0.099\n",
      "reward -0.095\n",
      "reward -0.100\n",
      "reward -0.117\n",
      "reward -0.989\n",
      "reward 0.313\n",
      "reward 0.255\n",
      "reward -0.923\n",
      "reward -1.086\n",
      "reward -1.709\n",
      "reward -0.588\n",
      "reward -1.965\n",
      "reward -2.642\n",
      "reward 29.407\n",
      "reward 6.043\n",
      "reward -100.000\n",
      "timesteps 74 reward -121.73\n",
      "reward -0.896\n",
      "reward -0.889\n",
      "reward -0.886\n",
      "reward -1.798\n",
      "reward -1.764\n",
      "reward -1.727\n",
      "reward -1.687\n",
      "reward -1.644\n",
      "reward -1.600\n",
      "reward -1.553\n",
      "reward -1.403\n",
      "reward -1.696\n",
      "reward -1.850\n",
      "reward -1.797\n",
      "reward -1.743\n",
      "reward -1.689\n",
      "reward -1.675\n",
      "reward -1.748\n",
      "reward -1.692\n",
      "reward -1.636\n",
      "reward -1.562\n",
      "reward -1.278\n",
      "reward -1.057\n",
      "reward -1.001\n",
      "reward -1.039\n",
      "reward -0.753\n",
      "reward -0.501\n",
      "reward -0.297\n",
      "reward -0.241\n",
      "reward -0.185\n",
      "reward -0.277\n",
      "reward -0.235\n",
      "reward -0.179\n",
      "reward -0.123\n",
      "reward -0.067\n",
      "reward 0.096\n",
      "reward 0.277\n",
      "reward 0.331\n",
      "reward 0.385\n",
      "reward 0.437\n",
      "reward 0.489\n",
      "reward 0.539\n",
      "reward 0.588\n",
      "reward 0.635\n",
      "reward 0.678\n",
      "reward 0.718\n",
      "reward 0.752\n",
      "reward 0.778\n",
      "reward 0.792\n",
      "reward 0.790\n",
      "reward 0.761\n",
      "reward 0.688\n",
      "reward 0.541\n",
      "reward 0.264\n",
      "reward -0.242\n",
      "reward -1.109\n",
      "reward 7.622\n",
      "reward -100.000\n",
      "timesteps 58 reward -123.36\n",
      "reward -1.084\n",
      "reward -1.220\n",
      "reward -1.218\n",
      "reward -1.214\n",
      "reward -1.207\n",
      "reward -1.198\n",
      "reward -1.186\n",
      "reward -1.171\n",
      "reward -1.154\n",
      "reward -1.135\n",
      "reward -1.113\n",
      "reward -1.090\n",
      "reward -1.065\n",
      "reward -1.039\n",
      "reward -1.011\n",
      "reward -0.982\n",
      "reward -1.975\n",
      "reward -2.157\n",
      "reward -1.337\n",
      "reward -1.305\n",
      "reward -1.273\n",
      "reward -1.240\n",
      "reward -0.229\n",
      "reward -0.937\n",
      "reward -0.200\n",
      "reward -0.087\n",
      "reward 0.060\n",
      "reward 0.640\n",
      "reward 0.928\n",
      "reward 0.938\n",
      "reward 1.036\n",
      "reward 1.546\n",
      "reward 1.723\n",
      "reward 1.963\n",
      "reward 2.127\n",
      "reward 2.266\n",
      "reward 2.285\n",
      "reward 2.660\n",
      "reward 2.844\n",
      "reward 2.970\n",
      "reward 3.029\n",
      "reward 2.725\n",
      "reward 2.684\n",
      "reward -0.818\n",
      "reward -2.443\n",
      "reward -2.556\n",
      "reward -2.707\n",
      "reward -2.867\n",
      "reward -3.442\n",
      "reward 1.599\n",
      "reward 0.451\n",
      "reward -0.993\n",
      "reward 0.230\n",
      "reward 9.570\n",
      "reward -100.000\n",
      "timesteps 55 reward -100.38\n",
      "reward -1.023\n",
      "reward -1.176\n",
      "reward -1.181\n",
      "reward -1.184\n",
      "reward -1.185\n",
      "reward -1.184\n",
      "reward 0.964\n",
      "reward 2.440\n",
      "reward -0.273\n",
      "reward -1.619\n",
      "reward 0.540\n",
      "reward -0.419\n",
      "reward -1.164\n",
      "reward -1.847\n",
      "reward 0.926\n",
      "reward -1.275\n",
      "reward -0.109\n",
      "reward -2.531\n",
      "reward 0.204\n",
      "reward -2.120\n",
      "reward -2.326\n",
      "reward -1.558\n",
      "reward -0.711\n",
      "reward -0.180\n",
      "reward 0.039\n",
      "reward 0.517\n",
      "reward -0.583\n",
      "reward 0.405\n",
      "reward 0.875\n",
      "reward 0.757\n",
      "reward 0.846\n",
      "reward 1.258\n",
      "reward 1.147\n",
      "reward 1.271\n",
      "reward 1.381\n",
      "reward 1.987\n",
      "reward 2.195\n",
      "reward 3.714\n",
      "reward 0.706\n",
      "reward 1.628\n",
      "reward 3.218\n",
      "reward 1.123\n",
      "reward 2.587\n",
      "reward 1.942\n",
      "reward 2.389\n",
      "reward 1.486\n",
      "reward -2.271\n",
      "reward -2.406\n",
      "reward -2.719\n",
      "reward -3.102\n",
      "reward -3.267\n",
      "reward -3.497\n",
      "reward -3.659\n",
      "reward -3.869\n",
      "reward -4.323\n",
      "reward -4.582\n",
      "reward -4.740\n",
      "reward -5.188\n",
      "reward -5.350\n",
      "reward -5.710\n",
      "reward -5.994\n",
      "reward -6.201\n",
      "reward 0.996\n",
      "reward -2.788\n",
      "reward 1.117\n",
      "reward 0.900\n",
      "reward -4.000\n",
      "reward -6.212\n",
      "reward -5.962\n",
      "reward -5.594\n",
      "reward -5.287\n",
      "reward -4.956\n",
      "reward -4.759\n",
      "reward -4.539\n",
      "reward -4.348\n",
      "reward -4.261\n",
      "reward -3.973\n",
      "reward -3.589\n",
      "reward -3.346\n",
      "reward -3.299\n",
      "reward -3.047\n",
      "reward -2.979\n",
      "reward -3.526\n",
      "reward -3.628\n",
      "reward -3.739\n",
      "reward -3.858\n",
      "reward -3.987\n",
      "reward -4.123\n",
      "reward -1.922\n",
      "reward -2.517\n",
      "reward -3.955\n",
      "reward -100.000\n",
      "timesteps 92 reward -255.16\n",
      "reward -1.476\n",
      "reward -1.643\n",
      "reward -1.698\n",
      "reward -1.732\n",
      "reward -1.750\n",
      "reward -1.754\n",
      "reward -1.746\n",
      "reward -1.731\n",
      "reward -1.708\n",
      "reward -2.303\n",
      "reward -2.468\n",
      "reward -1.950\n",
      "reward -1.913\n",
      "reward -1.873\n",
      "reward -1.831\n",
      "reward -1.786\n",
      "reward -1.164\n",
      "reward -0.808\n",
      "reward -1.194\n",
      "reward -1.142\n",
      "reward -0.661\n",
      "reward -0.876\n",
      "reward -0.822\n",
      "reward -0.767\n",
      "reward -0.379\n",
      "reward -0.454\n",
      "reward -0.398\n",
      "reward 0.049\n",
      "reward 0.374\n",
      "reward 0.548\n",
      "reward 0.893\n",
      "reward 1.087\n",
      "reward 0.864\n",
      "reward -1.957\n",
      "reward -1.989\n",
      "reward -1.931\n",
      "reward 1.155\n",
      "reward 3.498\n",
      "reward 2.220\n",
      "reward -1.984\n",
      "reward -1.926\n",
      "reward 3.912\n",
      "reward 3.897\n",
      "reward 2.620\n",
      "reward 0.782\n",
      "reward -2.096\n",
      "reward -1.789\n",
      "reward -1.477\n",
      "reward -1.250\n",
      "reward -1.313\n",
      "reward -1.030\n",
      "reward -0.739\n",
      "reward -0.441\n",
      "reward -0.342\n",
      "reward 0.051\n",
      "reward 0.235\n",
      "reward 0.556\n",
      "reward 0.461\n",
      "reward 3.621\n",
      "reward 2.993\n",
      "reward 2.350\n",
      "reward 4.377\n",
      "reward 4.800\n",
      "reward 3.744\n",
      "reward 2.924\n",
      "reward 5.169\n",
      "reward -0.703\n",
      "reward 8.758\n",
      "reward 5.775\n",
      "reward -100.000\n",
      "timesteps 70 reward -91.28\n",
      "reward -1.363\n",
      "reward -1.411\n",
      "reward -1.348\n",
      "reward -1.287\n",
      "reward -1.227\n",
      "reward -1.167\n",
      "reward -1.107\n",
      "reward -1.047\n",
      "reward -0.987\n",
      "reward -0.927\n",
      "reward -0.867\n",
      "reward -0.807\n",
      "reward -0.747\n",
      "reward -0.687\n",
      "reward -0.627\n",
      "reward -0.567\n",
      "reward -0.507\n",
      "reward -0.447\n",
      "reward -0.387\n",
      "reward -0.327\n",
      "reward -0.267\n",
      "reward -0.207\n",
      "reward -0.147\n",
      "reward -0.087\n",
      "reward -0.121\n",
      "reward 0.035\n",
      "reward 0.095\n",
      "reward 0.155\n",
      "reward 0.215\n",
      "reward 0.275\n",
      "reward 0.335\n",
      "reward 0.395\n",
      "reward 0.455\n",
      "reward 0.515\n",
      "reward 0.575\n",
      "reward 0.635\n",
      "reward 0.695\n",
      "reward 0.755\n",
      "reward 0.815\n",
      "reward 0.875\n",
      "reward 0.934\n",
      "reward 0.994\n",
      "reward 1.054\n",
      "reward 1.114\n",
      "reward 1.174\n",
      "reward 1.234\n",
      "timesteps 46 reward -5.35\n",
      "reward 0.839\n",
      "reward 0.909\n",
      "reward 0.925\n",
      "reward 0.930\n",
      "reward 0.927\n",
      "reward 0.916\n",
      "reward 0.894\n",
      "reward 0.861\n",
      "reward 0.815\n",
      "reward 0.755\n",
      "reward 0.678\n",
      "reward 0.584\n",
      "reward 0.472\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys, gym, time\n",
    "\n",
    "#\n",
    "# Test yourself as a learning agent! Pass environment name as a command-line argument, for example:\n",
    "#\n",
    "# python keyboard_agent.py SpaceInvadersNoFrameskip-v4\n",
    "#\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "if not hasattr(env.action_space, 'n'):\n",
    "    raise Exception('Keyboard agent only supports discrete action spaces')\n",
    "ACTIONS = env.action_space.n\n",
    "SKIP_CONTROL = 0    # Use previous control decision SKIP_CONTROL times, that's how you\n",
    "                    # can test what skip is still usable.\n",
    "\n",
    "human_agent_action = 0\n",
    "human_wants_restart = False\n",
    "human_sets_pause = False\n",
    "\n",
    "def key_press(key, mod):\n",
    "    global human_agent_action, human_wants_restart, human_sets_pause\n",
    "    if key==0xff0d: human_wants_restart = True\n",
    "    if key==32: human_sets_pause = not human_sets_pause\n",
    "    a = int( key - ord('0') )\n",
    "    if a <= 0 or a >= ACTIONS: return\n",
    "    human_agent_action = a\n",
    "\n",
    "def key_release(key, mod):\n",
    "    global human_agent_action\n",
    "    a = int( key - ord('0') )\n",
    "    if a <= 0 or a >= ACTIONS: return\n",
    "    if human_agent_action == a:\n",
    "        human_agent_action = 0\n",
    "\n",
    "env.render()\n",
    "env.unwrapped.viewer.window.on_key_press = key_press\n",
    "env.unwrapped.viewer.window.on_key_release = key_release\n",
    "\n",
    "def rollout(env):\n",
    "    global human_agent_action, human_wants_restart, human_sets_pause\n",
    "    human_wants_restart = False\n",
    "    obser = env.reset()\n",
    "    skip = 0\n",
    "    total_reward = 0\n",
    "    total_timesteps = 0\n",
    "    while 1:\n",
    "        if not skip:\n",
    "            #print(\"taking action {}\".format(human_agent_action))\n",
    "            a = human_agent_action\n",
    "            total_timesteps += 1\n",
    "            skip = SKIP_CONTROL\n",
    "        else:\n",
    "            skip -= 1\n",
    "\n",
    "        obser, r, done, info = env.step(a)\n",
    "        if r != 0:\n",
    "            print(\"reward %0.3f\" % r)\n",
    "        total_reward += r\n",
    "        window_still_open = env.render()\n",
    "        if window_still_open==False: return False\n",
    "        if done: break\n",
    "        if human_wants_restart: break\n",
    "        while human_sets_pause:\n",
    "            env.render()\n",
    "            time.sleep(0.1)\n",
    "        time.sleep(0.1)\n",
    "    print(\"timesteps %i reward %0.2f\" % (total_timesteps, total_reward))\n",
    "\n",
    "print(\"ACTIONS={}\".format(ACTIONS))\n",
    "print(\"Press keys 1 2 3 ... to take actions 1 2 3 ...\")\n",
    "print(\"No keys pressed is taking action 0\")\n",
    "\n",
    "while 1:\n",
    "    window_still_open = rollout(env)\n",
    "    if window_still_open==False: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
