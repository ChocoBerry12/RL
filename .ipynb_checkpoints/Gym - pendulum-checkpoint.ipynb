{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum\n",
    "---\n",
    ">action 과 state 가 모두 continuous 한 환경이다. 목표는 막대를 수직으로 오래 유지하는 것이다. 종료조건은 따로 없다. 최대 시간을 두는 것이 좋을 수도 있다.\n",
    "\n",
    "\n",
    "* state : 막대의 sin, cos, 각속도\n",
    "* action : 조인트에 작용하는 -2 ~ 2 사이 토크\n",
    "* reward : -(theta^2 + 0.1*theta_dt^2 + 0.001*action^2)  \n",
    "https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow  \n",
    "https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "환경 생성\n",
    "'''\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "size_s = env.observation_space.shape[0]\n",
    "size_out = env.action_space.shape[0]\n",
    "gamma = .95\n",
    "print(size_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "학습 네트워크 -> target 네트워크 로 복사하는 함수\n",
    "''' \n",
    "def copy(*, dest_scope_name = 'target', src_scope_name = 'main') :\n",
    "    op_holder = []\n",
    "    \n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
    "    \n",
    "    for src_var, dest_var in zip(src_vars, dest_vars) :\n",
    "        op_holder.append(dest_var.assign(src_var))\n",
    "        \n",
    "    return op_holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "replay experience, 공급된 batch 로 학습\n",
    "'''\n",
    "def replay_train(mainDQN, targetDQN, train_batch) :\n",
    "    x_stack = np.empty(0).reshape(0, size_s)\n",
    "    y_stack = np.empty(0).reshape(0, size_out)\n",
    "    \n",
    "    for state, action, reward, next_state, done in train_batch :\n",
    "        \n",
    "        # 현재 Q 는 main 네트워크에서 도출\n",
    "        Q = mainDQN.predict(state)\n",
    "        \n",
    "        # terminal step 이면 음의 보상\n",
    "        if done :\n",
    "            Q[0, 0] = -100\n",
    "        \n",
    "        # target Q 는 target 네트워크에서 도출 - 자기상관도 감소 (Double Q 러닝 기법이 포함된듯)\n",
    "        else :\n",
    "            Q[0, 0] = reward + gamma*np.max(targetDQN.predict(next_state))\n",
    "        \n",
    "        # x 는 현재 Q\n",
    "        # y 는 target\n",
    "        x_stack = np.vstack([x_stack, state])\n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        \n",
    "    return mainDQN.update(x_stack, y_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable main/W1/Adam/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-bab9a7d305bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;33m-\u001b[0m \u001b[0m다른\u001b[0m \u001b[0m하나는\u001b[0m \u001b[0m일시적으로\u001b[0m \u001b[0m고정되어\u001b[0m \u001b[0m학습의\u001b[0m \u001b[0m목표가\u001b[0m \u001b[0m되는\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0m네트워크\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     '''\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mmainDQN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'main'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mtargetDQN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\RL\\dqn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session, input_size, output_size, h1_size, h2_size, name)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# 네트워크를 위한 식 만들기 (레이어는 어떻게 구성되어 있으며, 활성화 함수는 어떤 것을 사용할지)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\RL\\dqn.py\u001b[0m in \u001b[0;36m_build_network\u001b[1;34m(self, h1_size, h2_size, l_rate)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Y\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Qpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# x에 대한 y 결과 리턴 (2단계 네트워크 지나온 결과) -> 관계식을 적용한 결과\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[1;32m--> 410\u001b[1;33m                                 name=name)\n\u001b[0m\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    591\u001b[0m                        ([str(v) for _, v, _ in converted_grads_and_vars],))\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\u001b[0m in \u001b[0;36m_create_slots\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# Create slots for the first and second moments.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"m\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"v\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36m_zeros_slot\u001b[1;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[0mnew_slot_variable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m       self._restore_slot_variable(\n\u001b[0;32m   1141\u001b[0m           \u001b[0mslot_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[1;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[0;32m    181\u001b[0m     return create_slot_with_initializer(\n\u001b[0;32m    182\u001b[0m         \u001b[0mprimary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslot_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         colocate_with_primary=colocate_with_primary)\n\u001b[0m\u001b[0;32m    184\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot_with_initializer\u001b[1;34m(primary, initializer, shape, dtype, name, colocate_with_primary)\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_vars_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n\u001b[1;32m--> 157\u001b[1;33m                                 dtype)\n\u001b[0m\u001b[0;32m    158\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m       return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36m_create_slot_var\u001b[1;34m(primary, val, scope, validate_shape, shape, dtype)\u001b[0m\n\u001b[0;32m     63\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresource_variable_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_resource_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m       \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m       validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m     66\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1485\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1487\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1235\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1237\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    538\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    490\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    859\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 861\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    862\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable main/W1/Adam/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "total_episode = 1000\n",
    "replay_buffer = deque()\n",
    "ep = 0\n",
    "step = 0\n",
    "max_step = 1000\n",
    "#saver = tf.train.Saver()\n",
    "#save_file = 'C:\\\\Users\\\\김민수\\\\Documents\\\\GitHub\\\\RL\\\\vars\\\\cartpole_dqn'\n",
    "mean_reward = deque()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "      \n",
    "    '''\n",
    "    - 네트워크 2 개 생성\n",
    "    - 하나는 실제 학습을 하는 학습 네트워크\n",
    "    - 다른 하나는 일시적으로 고정되어 학습의 목표가 되는 target 네트워크\n",
    "    '''\n",
    "    mainDQN = dqn.DQN(sess, size_s, size_out, 8, 6, name='main')\n",
    "    targetDQN = dqn.DQN(sess, size_s, size_out, 8, 6, name='target')\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # 처음 시작 시 네트워크 복사\n",
    "    copy_ops = copy(dest_scope_name='targetDQN', src_scope_name='mainDQN')\n",
    "    sess.run(copy_ops)\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for episode in range(total_episode) :\n",
    "        e = 1. / ((episode / 20) + 1)    # 점점 감소하는 explore \n",
    "        done = False\n",
    "        step_count = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not done :\n",
    "             \n",
    "            # 액션 선택\n",
    "            if np.random.rand(1) < e :\n",
    "                action = env.action_space.sample()\n",
    "            else :\n",
    "                action = (mainDQN.predict(state)) * 2\n",
    "                    \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "            if(step_count > 1000):\n",
    "                done = True\n",
    "                \n",
    "                \n",
    "            # 학습하지않고 임시메모리에 저장한다.\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if(len(replay_buffer) > 50000) :\n",
    "                replay_buffer.popleft()\n",
    "            \n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            \n",
    "            if(step_count > 1000) :\n",
    "                break\n",
    "        \n",
    "        \n",
    "        print ('episode : {}, steps : {}'.format(episode, step_count))\n",
    "                \n",
    "        '''\n",
    "        파라미터 로컬에 저장.\n",
    "        '''\n",
    "        if(len(mean_reward) < 10):\n",
    "            mean_reward.append(step_count)\n",
    "        else:\n",
    "            mean_reward.popleft()\n",
    "            \n",
    "            '''\n",
    "            if(np.mean(mean_reward) > 800):\n",
    "                saver.save(sess, save_file)\n",
    "                break\n",
    "            '''\n",
    "                \n",
    "        # 10 회의 에피소드가 끝나면 임시메모리에서 과거 데이터를 랜덤으로 뽑아 학습한다.\n",
    "        if(episode % 10 == 1) :\n",
    "            for _ in range(50) :\n",
    "                minibacth = random.sample(replay_buffer, 10)\n",
    "                loss, _ = replay_train(mainDQN, targetDQN, minibacth)\n",
    "                        \n",
    "            print('loss :', loss)\n",
    "            sess.run(copy_ops)   # 목표가 되는 네트워크를 업데이트한다.\n",
    "                    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size: \n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        '''     \n",
    "        batch_size specifies the number of experiences to add \n",
    "        to the batch. If the replay buffer has less than batch_size\n",
    "        elements, simply return all of the elements within the buffer.\n",
    "        Generally, you'll want to wait until the buffer has at least \n",
    "        batch_size elements before beginning to sample from it.\n",
    "        '''\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\김민수\\Documents\\GitHub\\RL\\RL_brain.py:265: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dueling DQN & Natural DQN comparison\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "Tensorflow: 1.0\n",
    "gym: 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "import gym\n",
    "from RL_brain import DuelingDQN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "MEMORY_SIZE = 3000\n",
    "ACTION_SPACE = 25\n",
    "\n",
    "sess = tf.Session()\n",
    "with tf.variable_scope('natural'):\n",
    "    natural_DQN = DuelingDQN(\n",
    "        n_actions=ACTION_SPACE, n_features=3, memory_size=MEMORY_SIZE,\n",
    "        e_greedy_increment=0.001, sess=sess, dueling=False)\n",
    "\n",
    "with tf.variable_scope('dueling'):\n",
    "    dueling_DQN = DuelingDQN(\n",
    "        n_actions=ACTION_SPACE, n_features=3, memory_size=MEMORY_SIZE,\n",
    "        e_greedy_increment=0.001, sess=sess, dueling=True, output_graph=True)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "def train(RL):\n",
    "    acc_r = [0]\n",
    "    total_steps = 0\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        # if total_steps-MEMORY_SIZE > 9000: env.render()\n",
    "\n",
    "        action = RL.choose_action(observation)\n",
    "\n",
    "        f_action = (action-(ACTION_SPACE-1)/2)/((ACTION_SPACE-1)/4)   # [-2 ~ 2] float actions\n",
    "        observation_, reward, done, info = env.step(np.array([f_action]))\n",
    "\n",
    "        reward /= 10      # normalize to a range of (-1, 0)\n",
    "        acc_r.append(reward + acc_r[-1])  # accumulated reward\n",
    "\n",
    "        RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "        if total_steps > MEMORY_SIZE:\n",
    "            RL.learn()\n",
    "\n",
    "        if total_steps-MEMORY_SIZE > 15000:\n",
    "            break\n",
    "\n",
    "        observation = observation_\n",
    "        total_steps += 1\n",
    "    return RL.cost_his, acc_r\n",
    "\n",
    "c_natural, r_natural = train(natural_DQN)\n",
    "c_dueling, r_dueling = train(dueling_DQN)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(np.array(c_natural), c='r', label='natural')\n",
    "plt.plot(np.array(c_dueling), c='b', label='dueling')\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('training steps')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(np.array(r_natural), c='r', label='natural')\n",
    "plt.plot(np.array(r_dueling), c='b', label='dueling')\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('accumulated reward')\n",
    "plt.xlabel('training steps')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Reward: -1271 Explore: 3.00\n",
      "Episode: 1  Reward: -1300 Explore: 3.00\n",
      "Episode: 2  Reward: -1238 Explore: 3.00\n",
      "Episode: 3  Reward: -1480 Explore: 3.00\n",
      "Episode: 4  Reward: -1423 Explore: 3.00\n",
      "Episode: 5  Reward: -1664 Explore: 3.00\n",
      "Episode: 6  Reward: -1115 Explore: 3.00\n",
      "Episode: 7  Reward: -1179 Explore: 3.00\n",
      "Episode: 8  Reward: -1168 Explore: 3.00\n",
      "Episode: 9  Reward: -1043 Explore: 3.00\n",
      "Episode: 10  Reward: -1261 Explore: 3.00\n",
      "Episode: 11  Reward: -1133 Explore: 3.00\n",
      "Episode: 12  Reward: -1810 Explore: 3.00\n",
      "Episode: 13  Reward: -1196 Explore: 3.00\n",
      "Episode: 14  Reward: -1274 Explore: 3.00\n",
      "Episode: 15  Reward: -1518 Explore: 3.00\n",
      "Episode: 16  Reward: -1331 Explore: 3.00\n",
      "Episode: 17  Reward: -1247 Explore: 3.00\n",
      "Episode: 18  Reward: -1338 Explore: 3.00\n",
      "Episode: 19  Reward: -1413 Explore: 3.00\n",
      "Episode: 20  Reward: -1605 Explore: 3.00\n",
      "Episode: 21  Reward: -1197 Explore: 3.00\n",
      "Episode: 22  Reward: -1340 Explore: 3.00\n",
      "Episode: 23  Reward: -1686 Explore: 3.00\n",
      "Episode: 24  Reward: -1367 Explore: 3.00\n",
      "Episode: 25  Reward: -1251 Explore: 3.00\n",
      "Episode: 26  Reward: -1128 Explore: 3.00\n",
      "Episode: 27  Reward: -1170 Explore: 3.00\n",
      "Episode: 28  Reward: -1274 Explore: 3.00\n",
      "Episode: 29  Reward: -1129 Explore: 3.00\n",
      "Episode: 30  Reward: -1269 Explore: 3.00\n",
      "Episode: 31  Reward: -1189 Explore: 3.00\n",
      "Episode: 32  Reward: -1325 Explore: 3.00\n",
      "Episode: 33  Reward: -1338 Explore: 3.00\n",
      "Episode: 34  Reward: -1199 Explore: 3.00\n",
      "Episode: 35  Reward: -1311 Explore: 3.00\n",
      "Episode: 36  Reward: -1202 Explore: 3.00\n",
      "Episode: 37  Reward: -1137 Explore: 3.00\n",
      "Episode: 38  Reward: -1092 Explore: 3.00\n",
      "Episode: 39  Reward: -1177 Explore: 3.00\n",
      "Episode: 40  Reward: -1298 Explore: 3.00\n",
      "Episode: 41  Reward: -1250 Explore: 3.00\n",
      "Episode: 42  Reward: -1056 Explore: 3.00\n",
      "Episode: 43  Reward: -1178 Explore: 3.00\n",
      "Episode: 44  Reward: -1235 Explore: 3.00\n",
      "Episode: 45  Reward: -1513 Explore: 3.00\n",
      "Episode: 46  Reward: -1344 Explore: 3.00\n",
      "Episode: 47  Reward: -1633 Explore: 3.00\n",
      "Episode: 48  Reward: -1289 Explore: 3.00\n",
      "Episode: 49  Reward: -1595 Explore: 3.00\n",
      "Episode: 50  Reward: -1312 Explore: 2.71\n",
      "Episode: 51  Reward: -1295 Explore: 2.46\n",
      "Episode: 52  Reward: -1325 Explore: 2.22\n",
      "Episode: 53  Reward: -1298 Explore: 2.01\n",
      "Episode: 54  Reward: -1608 Explore: 1.82\n",
      "Episode: 55  Reward: -1111 Explore: 1.65\n",
      "Episode: 56  Reward: -1309 Explore: 1.49\n",
      "Episode: 57  Reward: -1516 Explore: 1.35\n",
      "Episode: 58  Reward: -1505 Explore: 1.22\n",
      "Episode: 59  Reward: -1132 Explore: 1.10\n",
      "Episode: 60  Reward: -1265 Explore: 1.00\n",
      "Episode: 61  Reward: -1320 Explore: 0.90\n",
      "Episode: 62  Reward: -1470 Explore: 0.82\n",
      "Episode: 63  Reward: -1545 Explore: 0.74\n",
      "Episode: 64  Reward: -1251 Explore: 0.67\n",
      "Episode: 65  Reward: -1375 Explore: 0.61\n",
      "Episode: 66  Reward: -1169 Explore: 0.55\n",
      "Episode: 67  Reward: -1549 Explore: 0.50\n",
      "Episode: 68  Reward: -1554 Explore: 0.45\n",
      "Episode: 69  Reward: -1535 Explore: 0.41\n",
      "Episode: 70  Reward: -1633 Explore: 0.37\n",
      "Episode: 71  Reward: -1547 Explore: 0.33\n",
      "Episode: 72  Reward: -1287 Explore: 0.30\n",
      "Episode: 73  Reward: -1161 Explore: 0.27\n",
      "Episode: 74  Reward: -1628 Explore: 0.25\n",
      "Episode: 75  Reward: -1095 Explore: 0.22\n",
      "Episode: 76  Reward: -1382 Explore: 0.20\n",
      "Episode: 77  Reward: -1647 Explore: 0.18\n",
      "Episode: 78  Reward: -1514 Explore: 0.16\n",
      "Episode: 79  Reward: -1211 Explore: 0.15\n",
      "Episode: 80  Reward: -1513 Explore: 0.14\n",
      "Episode: 81  Reward: -1543 Explore: 0.12\n",
      "Episode: 82  Reward: -1501 Explore: 0.11\n",
      "Episode: 83  Reward: -1459 Explore: 0.10\n",
      "Episode: 84  Reward: -945 Explore: 0.09\n",
      "Episode: 85  Reward: -1616 Explore: 0.08\n",
      "Episode: 86  Reward: -1642 Explore: 0.07\n",
      "Episode: 87  Reward: -1639 Explore: 0.07\n",
      "Episode: 88  Reward: -1647 Explore: 0.06\n",
      "Episode: 89  Reward: -1189 Explore: 0.05\n",
      "Episode: 90  Reward: -1499 Explore: 0.05\n",
      "Episode: 91  Reward: -1158 Explore: 0.04\n",
      "Episode: 92  Reward: -1332 Explore: 0.04\n",
      "Episode: 93  Reward: -1654 Explore: 0.04\n",
      "Episode: 94  Reward: -1549 Explore: 0.03\n",
      "Episode: 95  Reward: -1339 Explore: 0.03\n",
      "Episode: 96  Reward: -1646 Explore: 0.03\n",
      "Episode: 97  Reward: -1058 Explore: 0.02\n",
      "Episode: 98  Reward: -721 Explore: 0.02\n",
      "Episode: 99  Reward: -1474 Explore: 0.02\n",
      "Episode: 100  Reward: -1525 Explore: 0.02\n",
      "Episode: 101  Reward: -1631 Explore: 0.02\n",
      "Episode: 102  Reward: -1494 Explore: 0.01\n",
      "Episode: 103  Reward: -1607 Explore: 0.01\n",
      "Episode: 104  Reward: -1608 Explore: 0.01\n",
      "Episode: 105  Reward: -1562 Explore: 0.01\n",
      "Episode: 106  Reward: -1520 Explore: 0.01\n",
      "Episode: 107  Reward: -1119 Explore: 0.01\n",
      "Episode: 108  Reward: -1365 Explore: 0.01\n",
      "Episode: 109  Reward: -1136 Explore: 0.01\n",
      "Episode: 110  Reward: -937 Explore: 0.01\n",
      "Episode: 111  Reward: -1278 Explore: 0.01\n",
      "Episode: 112  Reward: -1502 Explore: 0.01\n",
      "Episode: 113  Reward: -1616 Explore: 0.00\n",
      "Episode: 114  Reward: -1537 Explore: 0.00\n",
      "Episode: 115  Reward: -1476 Explore: 0.00\n",
      "Episode: 116  Reward: -1377 Explore: 0.00\n",
      "Episode: 117  Reward: -1502 Explore: 0.00\n",
      "Episode: 118  Reward: -1633 Explore: 0.00\n",
      "Episode: 119  Reward: -1315 Explore: 0.00\n",
      "Episode: 120  Reward: -1634 Explore: 0.00\n",
      "Episode: 121  Reward: -1302 Explore: 0.00\n",
      "Episode: 122  Reward: -1278 Explore: 0.00\n",
      "Episode: 123  Reward: -1621 Explore: 0.00\n",
      "Episode: 124  Reward: -1108 Explore: 0.00\n",
      "Episode: 125  Reward: -1207 Explore: 0.00\n",
      "Episode: 126  Reward: -1641 Explore: 0.00\n",
      "Episode: 127  Reward: -838 Explore: 0.00\n",
      "Episode: 128  Reward: -1551 Explore: 0.00\n",
      "Episode: 129  Reward: -1568 Explore: 0.00\n",
      "Episode: 130  Reward: -1540 Explore: 0.00\n",
      "Episode: 131  Reward: -1510 Explore: 0.00\n",
      "Episode: 132  Reward: -1292 Explore: 0.00\n",
      "Episode: 133  Reward: -1648 Explore: 0.00\n",
      "Episode: 134  Reward: -1490 Explore: 0.00\n",
      "Episode: 135  Reward: -1654 Explore: 0.00\n",
      "Episode: 136  Reward: -1503 Explore: 0.00\n",
      "Episode: 137  Reward: -1397 Explore: 0.00\n",
      "Episode: 138  Reward: -1345 Explore: 0.00\n",
      "Episode: 139  Reward: -1283 Explore: 0.00\n",
      "Episode: 140  Reward: -1286 Explore: 0.00\n",
      "Episode: 141  Reward: -1483 Explore: 0.00\n",
      "Episode: 142  Reward: -1651 Explore: 0.00\n",
      "Episode: 143  Reward: -954 Explore: 0.00\n",
      "Episode: 144  Reward: -1653 Explore: 0.00\n",
      "Episode: 145  Reward: -1010 Explore: 0.00\n",
      "Episode: 146  Reward: -1491 Explore: 0.00\n",
      "Episode: 147  Reward: -1360 Explore: 0.00\n",
      "Episode: 148  Reward: -1494 Explore: 0.00\n",
      "Episode: 149  Reward: -1196 Explore: 0.00\n",
      "Episode: 150  Reward: -1593 Explore: 0.00\n",
      "Episode: 151  Reward: -1494 Explore: 0.00\n",
      "Episode: 152  Reward: -1506 Explore: 0.00\n",
      "Episode: 153  Reward: -1569 Explore: 0.00\n",
      "Episode: 154  Reward: -1407 Explore: 0.00\n",
      "Episode: 155  Reward: -1133 Explore: 0.00\n",
      "Episode: 156  Reward: -1502 Explore: 0.00\n",
      "Episode: 157  Reward: -1330 Explore: 0.00\n",
      "Episode: 158  Reward: -1654 Explore: 0.00\n",
      "Episode: 159  Reward: -1438 Explore: 0.00\n",
      "Episode: 160  Reward: -1642 Explore: 0.00\n",
      "Episode: 161  Reward: -1504 Explore: 0.00\n",
      "Episode: 162  Reward: -1657 Explore: 0.00\n",
      "Episode: 163  Reward: -1637 Explore: 0.00\n",
      "Episode: 164  Reward: -1243 Explore: 0.00\n",
      "Episode: 165  Reward: -1053 Explore: 0.00\n",
      "Episode: 166  Reward: -1511 Explore: 0.00\n",
      "Episode: 167  Reward: -1623 Explore: 0.00\n",
      "Episode: 168  Reward: -1067 Explore: 0.00\n",
      "Episode: 169  Reward: -946 Explore: 0.00\n",
      "Episode: 170  Reward: -1433 Explore: 0.00\n",
      "Episode: 171  Reward: -1495 Explore: 0.00\n",
      "Episode: 172  Reward: -1656 Explore: 0.00\n",
      "Episode: 173  Reward: -1651 Explore: 0.00\n",
      "Episode: 174  Reward: -1656 Explore: 0.00\n",
      "Episode: 175  Reward: -1036 Explore: 0.00\n",
      "Episode: 176  Reward: -1629 Explore: 0.00\n",
      "Episode: 177  Reward: -1219 Explore: 0.00\n",
      "Episode: 178  Reward: -1218 Explore: 0.00\n",
      "Episode: 179  Reward: -1213 Explore: 0.00\n",
      "Episode: 180  Reward: -1652 Explore: 0.00\n",
      "Episode: 181  Reward: -1516 Explore: 0.00\n",
      "Episode: 182  Reward: -922 Explore: 0.00\n",
      "Episode: 183  Reward: -947 Explore: 0.00\n",
      "Episode: 184  Reward: -1389 Explore: 0.00\n",
      "Episode: 185  Reward: -1346 Explore: 0.00\n",
      "Episode: 186  Reward: -1112 Explore: 0.00\n",
      "Episode: 187  Reward: -1650 Explore: 0.00\n",
      "Episode: 188  Reward: -939 Explore: 0.00\n",
      "Episode: 189  Reward: -1638 Explore: 0.00\n",
      "Episode: 190  Reward: -1274 Explore: 0.00\n",
      "Episode: 191  Reward: -1495 Explore: 0.00\n",
      "Episode: 192  Reward: -1645 Explore: 0.00\n",
      "Episode: 193  Reward: -1533 Explore: 0.00\n",
      "Episode: 194  Reward: -1491 Explore: 0.00\n",
      "Episode: 195  Reward: -1533 Explore: 0.00\n",
      "Episode: 196  Reward: -1420 Explore: 0.00\n",
      "Episode: 197  Reward: -1625 Explore: 0.00\n",
      "Episode: 198  Reward: -1409 Explore: 0.00\n",
      "Episode: 199  Reward: -1480 Explore: 0.00\n",
      "Running time:  112.38993000984192\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep Deterministic Policy Gradient (DDPG), Reinforcement Learning.\n",
    "DDPG is Actor Critic based algorithm.\n",
    "Pendulum example.\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "tensorflow 1.0\n",
    "gym 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "MAX_EPISODES = 200\n",
    "MAX_EP_STEPS = 200\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.001    # learning rate for critic\n",
    "GAMMA = 0.9     # reward discount\n",
    "REPLACEMENT = [\n",
    "    dict(name='soft', tau=0.01),\n",
    "    dict(name='hard', rep_iter_a=600, rep_iter_c=500)\n",
    "][0]            # you can try different target replacement strategies\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "RENDER = False\n",
    "OUTPUT_GRAPH = True\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "\n",
    "###############################  Actor  ####################################\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, action_dim, action_bound, learning_rate, replacement):\n",
    "        self.sess = sess\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.lr = learning_rate\n",
    "        self.replacement = replacement\n",
    "        self.t_replace_counter = 0\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            # input s, output a\n",
    "            self.a = self._build_net(S, scope='eval_net', trainable=True)\n",
    "\n",
    "            # input s_, output a, get a_ for critic\n",
    "            self.a_ = self._build_net(S_, scope='target_net', trainable=False)\n",
    "\n",
    "        self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval_net')\n",
    "        self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target_net')\n",
    "\n",
    "        if self.replacement['name'] == 'hard':\n",
    "            self.t_replace_counter = 0\n",
    "            self.hard_replace = [tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)]\n",
    "        else:\n",
    "            self.soft_replace = [tf.assign(t, (1 - self.replacement['tau']) * t + self.replacement['tau'] * e)\n",
    "                                 for t, e in zip(self.t_params, self.e_params)]\n",
    "\n",
    "    def _build_net(self, s, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.3)\n",
    "            init_b = tf.constant_initializer(0.1)\n",
    "            net = tf.layers.dense(s, 30, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, name='l1',\n",
    "                                  trainable=trainable)\n",
    "            with tf.variable_scope('a'):\n",
    "                actions = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, kernel_initializer=init_w,\n",
    "                                          bias_initializer=init_b, name='a', trainable=trainable)\n",
    "                scaled_a = tf.multiply(actions, self.action_bound, name='scaled_a')  # Scale output to -action_bound to action_bound\n",
    "        return scaled_a\n",
    "\n",
    "    def learn(self, s):   # batch update\n",
    "        self.sess.run(self.train_op, feed_dict={S: s})\n",
    "\n",
    "        if self.replacement['name'] == 'soft':\n",
    "            self.sess.run(self.soft_replace)\n",
    "        else:\n",
    "            if self.t_replace_counter % self.replacement['rep_iter_a'] == 0:\n",
    "                self.sess.run(self.hard_replace)\n",
    "            self.t_replace_counter += 1\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]    # single state\n",
    "        return self.sess.run(self.a, feed_dict={S: s})[0]  # single action\n",
    "\n",
    "    def add_grad_to_graph(self, a_grads):\n",
    "        with tf.variable_scope('policy_grads'):\n",
    "            # ys = policy;\n",
    "            # xs = policy's parameters;\n",
    "            # a_grads = the gradients of the policy to get more Q\n",
    "            # tf.gradients will calculate dys/dxs with a initial gradients for ys, so this is dq/da * da/dparams\n",
    "            self.policy_grads = tf.gradients(ys=self.a, xs=self.e_params, grad_ys=a_grads)\n",
    "\n",
    "        with tf.variable_scope('A_train'):\n",
    "            opt = tf.train.AdamOptimizer(-self.lr)  # (- learning rate) for ascent policy\n",
    "            self.train_op = opt.apply_gradients(zip(self.policy_grads, self.e_params))\n",
    "\n",
    "\n",
    "###############################  Critic  ####################################\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, gamma, replacement, a, a_):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.replacement = replacement\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # Input (s, a), output q\n",
    "            self.a = tf.stop_gradient(a)    # stop critic update flows to actor\n",
    "            self.q = self._build_net(S, self.a, 'eval_net', trainable=True)\n",
    "\n",
    "            # Input (s_, a_), output q_ for q_target\n",
    "            self.q_ = self._build_net(S_, a_, 'target_net', trainable=False)    # target_q is based on a_ from Actor's target_net\n",
    "\n",
    "            self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval_net')\n",
    "            self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target_net')\n",
    "\n",
    "        with tf.variable_scope('target_q'):\n",
    "            self.target_q = R + self.gamma * self.q_\n",
    "\n",
    "        with tf.variable_scope('TD_error'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.target_q, self.q))\n",
    "\n",
    "        with tf.variable_scope('C_train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        with tf.variable_scope('a_grad'):\n",
    "            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)\n",
    "\n",
    "        if self.replacement['name'] == 'hard':\n",
    "            self.t_replace_counter = 0\n",
    "            self.hard_replacement = [tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)]\n",
    "        else:\n",
    "            self.soft_replacement = [tf.assign(t, (1 - self.replacement['tau']) * t + self.replacement['tau'] * e)\n",
    "                                     for t, e in zip(self.t_params, self.e_params)]\n",
    "\n",
    "    def _build_net(self, s, a, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.1)\n",
    "            init_b = tf.constant_initializer(0.1)\n",
    "\n",
    "            with tf.variable_scope('l1'):\n",
    "                n_l1 = 30\n",
    "                w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], initializer=init_w, trainable=trainable)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=init_b, trainable=trainable)\n",
    "                net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "\n",
    "            with tf.variable_scope('q'):\n",
    "                q = tf.layers.dense(net, 1, kernel_initializer=init_w, bias_initializer=init_b, trainable=trainable)   # Q(s,a)\n",
    "        return q\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.sess.run(self.train_op, feed_dict={S: s, self.a: a, R: r, S_: s_})\n",
    "        if self.replacement['name'] == 'soft':\n",
    "            self.sess.run(self.soft_replacement)\n",
    "        else:\n",
    "            if self.t_replace_counter % self.replacement['rep_iter_c'] == 0:\n",
    "                self.sess.run(self.hard_replacement)\n",
    "            self.t_replace_counter += 1\n",
    "\n",
    "\n",
    "#####################  Memory  ####################\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity, dims):\n",
    "        self.capacity = capacity\n",
    "        self.data = np.zeros((capacity, dims))\n",
    "        self.pointer = 0\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        index = self.pointer % self.capacity  # replace the old memory with new memory\n",
    "        self.data[index, :] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "    def sample(self, n):\n",
    "        assert self.pointer >= self.capacity, 'Memory has not been fulfilled'\n",
    "        indices = np.random.choice(self.capacity, size=n)\n",
    "        return self.data[indices, :]\n",
    "\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = env.action_space.high\n",
    "\n",
    "# all placeholder for tf\n",
    "with tf.name_scope('S'):\n",
    "    S = tf.placeholder(tf.float32, shape=[None, state_dim], name='s')\n",
    "with tf.name_scope('R'):\n",
    "    R = tf.placeholder(tf.float32, [None, 1], name='r')\n",
    "with tf.name_scope('S_'):\n",
    "    S_ = tf.placeholder(tf.float32, shape=[None, state_dim], name='s_')\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Create actor and critic.\n",
    "# They are actually connected to each other, details can be seen in tensorboard or in this picture:\n",
    "actor = Actor(sess, action_dim, action_bound, LR_A, REPLACEMENT)\n",
    "critic = Critic(sess, state_dim, action_dim, LR_C, GAMMA, REPLACEMENT, actor.a, actor.a_)\n",
    "actor.add_grad_to_graph(critic.a_grads)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "M = Memory(MEMORY_CAPACITY, dims=2 * state_dim + action_dim + 1)\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "var = 3  # control exploration\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(MAX_EPISODES):\n",
    "    s = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        # Add exploration noise\n",
    "        a = actor.choose_action(s)\n",
    "        a = np.clip(np.random.normal(a, var), -2, 2)    # add randomness to action selection for exploration\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        M.store_transition(s, a, r / 10, s_)\n",
    "\n",
    "        if M.pointer > MEMORY_CAPACITY:\n",
    "            var *= .9995    # decay the action randomness\n",
    "            b_M = M.sample(BATCH_SIZE)\n",
    "            b_s = b_M[:, :state_dim]\n",
    "            b_a = b_M[:, state_dim: state_dim + action_dim]\n",
    "            b_r = b_M[:, -state_dim - 1: -state_dim]\n",
    "            b_s_ = b_M[:, -state_dim:]\n",
    "\n",
    "            critic.learn(b_s, b_a, b_r, b_s_)\n",
    "            actor.learn(b_s)\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "\n",
    "        if j == MAX_EP_STEPS-1:\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % var, )\n",
    "            if ep_reward > -300:\n",
    "                RENDER = True\n",
    "            break\n",
    "\n",
    "print('Running time: ', time.time()-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
