{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO - continuous action space\n",
    "> Continuous action spaces use normal/gaussian distribution. In this case the model output is the mean+std which define the normal distribution to use for the action selection. The action is sampled from this distribution. The probability is in this case the probabilty of the actions value under the given normal distribution (I don't know the math for that but you can look it up).\n",
    "---\n",
    "* 연속액션 환경에서는 정규분포로 action 을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "ALPHA = .0005\n",
    "EPSILON = 1\n",
    "T = 10 # T step 만큼 데이터 쌓고 학습할 것\n",
    "LAMBDA = .95\n",
    "K = 3\n",
    "GAMMA = .99\n",
    "e = .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 128)\n",
    "        self.fc_pi = nn.Linear(128, 32)\n",
    "        self.fc_pi2 = nn.Linear(32, 1)\n",
    "        self.fc_v = nn.Linear(128, 32)\n",
    "        self.fc_v_2 = nn.Linear(32, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), ALPHA)\n",
    "    \n",
    "    # 출력이 Normal 분포 -> sampling 을 해서 실수값 뽑아서 사용해야함!\n",
    "    def pi(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc_pi(x))\n",
    "        prob = torch.tanh(self.fc_pi2(x))\n",
    "        prob = torch.distributions.normal.Normal(prob, .1) # 평균 : prob, 분산 : .01\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc_v(x))\n",
    "        x = self.fc_v_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, data, optimizer):\n",
    "    s, a, r, s2, d, prob = batch_factory(data)\n",
    "\n",
    "    # epoch K 만큼\n",
    "    for i in range(K):\n",
    "        td_target = r + GAMMA * net.v(s2)\n",
    "        delta = td_target - net.v(s)\n",
    "        delta = delta.detach().numpy() # 1 step advantage\n",
    "        advantage_lst = []\n",
    "        advantage = 0.0\n",
    "\n",
    "        # GAE 계산\n",
    "        for delta_t in delta[::-1]:\n",
    "            advantage = GAMMA * LAMBDA * advantage + delta_t[0]\n",
    "            advantage_lst.append([advantage])\n",
    "        advantage_lst.reverse()\n",
    "        advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "        pi_a = net.pi(s).sample()\n",
    "        ratio = torch.exp(torch.log(pi_a) - torch.log(prob))\n",
    "\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - e, 1 + e) * advantage\n",
    "        loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(net.v(s) , td_target.detach())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_factory(memory):\n",
    "    s_, a_, r_, s2_, d_, prob_ = [], [], [], [], [], []\n",
    "    for s, a, r, s2, d, p in memory:\n",
    "        s_.append(s)\n",
    "        a_.append([a])\n",
    "        r = -100 if d else r\n",
    "        r_.append([r])\n",
    "        s2_.append(s2)\n",
    "        d = 0 if d else 1\n",
    "        d_.append([d])\n",
    "        prob_.append([p])\n",
    "        \n",
    "    s_ = torch.tensor(s_, dtype=torch.float)\n",
    "    a_ = torch.tensor(a_)\n",
    "    r_ = torch.tensor(r_, dtype=torch.float)\n",
    "    s2_ = torch.tensor(s2_, dtype=torch.float)\n",
    "    d_ = torch.tensor(d_, dtype=torch.float)\n",
    "    prob_ = torch.tensor(prob_)\n",
    "    \n",
    "    return s_, a_, r_, s2_, d_, prob_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 -1157.1429691764401\n",
      "20 -1233.3929367235355\n",
      "30 -1314.6250742529203\n",
      "40 -1330.8527113213102\n",
      "50 -1248.5087452695675\n",
      "60 -1251.7373624157722\n",
      "70 -1071.319313571381\n",
      "80 -1410.9258096711542\n",
      "90 -1268.5580070981673\n",
      "100 -1227.7331624886344\n",
      "110 -1414.0550779485536\n",
      "120 -1041.6981542676601\n",
      "130 -1231.6959874054576\n",
      "140 -1187.0722833527623\n",
      "150 -1203.4940719713588\n",
      "160 -1169.2936722129054\n",
      "170 -1335.7373703491457\n",
      "180 -1198.369998682712\n",
      "190 -1361.2995551870442\n",
      "200 -1161.136702768257\n",
      "210 -1170.2848164965103\n",
      "220 -1180.19540880028\n",
      "230 -1186.4087012620325\n",
      "240 -1192.7455058068442\n",
      "250 -1143.3313112258625\n",
      "260 -1193.0499145575227\n",
      "270 -1280.1364039316813\n",
      "280 -1305.2485586330693\n",
      "290 -1351.7704155629951\n",
      "300 -1295.0466268261966\n",
      "310 -1281.4252999164719\n",
      "320 -1303.6796514760933\n",
      "330 -1350.3125163941284\n",
      "340 -1261.575993741809\n",
      "350 -1193.768350708924\n",
      "360 -1340.740599517588\n",
      "370 -1415.4995044968775\n",
      "380 -1253.3477943511089\n",
      "390 -1106.0109162408958\n",
      "400 -1317.6243168363167\n",
      "410 -1212.0637208937123\n",
      "420 -1390.9861850336215\n",
      "430 -1112.735830274346\n",
      "440 -1376.500582531855\n",
      "450 -1232.8155935429022\n",
      "460 -1218.6407599043653\n",
      "470 -1193.1796064321748\n",
      "480 -1328.9032101233079\n",
      "490 -1384.1390464514973\n",
      "500 -1203.8271076833212\n",
      "510 -1130.854165241578\n",
      "520 -1337.4648317637013\n",
      "530 -1216.3082975058214\n",
      "540 -1127.3487582466573\n",
      "550 -1326.0839419913066\n",
      "560 -1302.0640678701106\n",
      "570 -1202.5409724272997\n",
      "580 -1201.7465425662087\n",
      "590 -1342.952521176448\n",
      "600 -1146.491065969794\n",
      "610 -1216.8929057965609\n",
      "620 -1191.9741841360378\n",
      "630 -1234.0831091152522\n",
      "640 -1243.765960325314\n",
      "650 -1220.1065803302515\n",
      "660 -1291.6313856146285\n",
      "670 -1298.5155540630597\n",
      "680 -1229.3061801924673\n",
      "690 -1310.5701459008578\n",
      "700 -1264.5139498751469\n",
      "710 -1257.420903322682\n",
      "720 -1396.243517646487\n",
      "730 -1270.036611643704\n",
      "740 -1252.9591991498896\n",
      "750 -1275.8770793341575\n",
      "760 -1273.9329218009286\n",
      "770 -1210.759671197523\n",
      "780 -1306.3889087342272\n",
      "790 -1333.6029123215676\n",
      "800 -1304.7562572108184\n",
      "810 -1237.9362911091162\n",
      "820 -1241.8330688945912\n",
      "830 -1244.6240979861163\n",
      "840 -1293.6977498572894\n",
      "850 -1193.3191446669316\n",
      "860 -1249.1989751834303\n",
      "870 -1461.7644446317368\n",
      "880 -1081.0642863912833\n",
      "890 -1333.9303814798113\n",
      "900 -1272.7615171672421\n",
      "910 -1174.5627075792459\n",
      "920 -1238.809941601441\n",
      "930 -1121.5200035744094\n",
      "940 -1364.6357223355494\n",
      "950 -1262.4727484215882\n",
      "960 -1224.7291745282637\n",
      "970 -1266.2502559279433\n",
      "980 -1277.6945594462086\n",
      "990 -1204.7194939440892\n",
      "1000 -1219.755299164041\n",
      "1010 -1368.2852795416022\n",
      "1020 -1360.0141149293654\n",
      "1030 -1306.899737694717\n",
      "1040 -1222.2455176266753\n",
      "1050 -1318.9746352405305\n",
      "1060 -1298.1140887464837\n",
      "1070 -1055.057786637706\n",
      "1080 -1268.2289744183101\n",
      "1090 -1281.921732374799\n",
      "1100 -1179.3326146084996\n",
      "1110 -1187.6289888804538\n",
      "1120 -1219.5833034539314\n",
      "1130 -1196.3054561095369\n",
      "1140 -1087.0745630741944\n",
      "1150 -1250.1938174106913\n",
      "1160 -1410.64949344649\n",
      "1170 -1167.2869587823202\n",
      "1180 -1184.920025616019\n",
      "1190 -1172.0217131436204\n",
      "1200 -1269.6170729272933\n",
      "1210 -1314.1247517167385\n",
      "1220 -1245.7200391304177\n",
      "1230 -1266.240039347092\n",
      "1240 -1156.8677702580744\n",
      "1250 -1110.393703962241\n",
      "1260 -1225.478506220393\n",
      "1270 -1095.0373129704526\n",
      "1280 -1179.9199333569313\n",
      "1290 -1322.3197627145894\n",
      "1300 -1304.4148551595456\n",
      "1310 -1229.8446409780158\n",
      "1320 -1261.3298112673274\n",
      "1330 -1337.8536024373766\n",
      "1340 -1266.7378919911487\n",
      "1350 -1102.7800430034404\n",
      "1360 -1234.5725267032144\n",
      "1370 -1263.6838643303875\n",
      "1380 -1175.7177119829555\n",
      "1390 -1302.631955718532\n",
      "1400 -1195.890892290223\n",
      "1410 -1206.0748115655006\n",
      "1420 -1245.6547170140734\n",
      "1430 -1162.8883486252284\n",
      "1440 -1213.2417038383367\n",
      "1450 -1138.974405080822\n",
      "1460 -1292.8818407810527\n",
      "1470 -1329.5432912828242\n",
      "1480 -1266.5925519610462\n",
      "1490 -1183.7868891761077\n",
      "1500 -1261.324778685212\n",
      "1510 -1260.6784236210715\n",
      "1520 -1140.8863564028331\n",
      "1530 -1281.8968544169666\n",
      "1540 -1251.619269105885\n"
     ]
    }
   ],
   "source": [
    "net = PPO()\n",
    "ep = 1\n",
    "total_ep = 10000\n",
    "gamma = .95\n",
    "total_reward = 0\n",
    "data = []\n",
    "epsilon = .1\n",
    "optimizer = optim.Adam(net.parameters(), ALPHA)\n",
    "\n",
    "while(ep < total_ep):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while(not done):\n",
    "        # T step 움직인 후 clipping - T 가 너무 크면 불안정??\n",
    "        for t in range(T):\n",
    "            prob = net.pi(torch.from_numpy(state).float())\n",
    "            action = env.action_space.sample()\n",
    "            #print(action)\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            data.append((state, action, reward/100.0, state_next, done, action))\n",
    "            state = state_next\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        train_net(net, data, optimizer)\n",
    "        data = []\n",
    "        \n",
    "    ep += 1\n",
    "    if(ep%10 == 0):\n",
    "        print(ep, total_reward/10.0)\n",
    "        total_reward = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
