{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole\n",
    "---\n",
    ">continous 한 action 환경 중에서 가장 단순한 환경이다. 매 프레임마다 reward 를 제공하는 dense reward 환경이라서 때문에 학습이 매우 용이하다. \n",
    "또한 terminal condition 은 카트가 일정 범위 넘어가거나 막대가 일정 각도 이상되는 것이기 때문에 명확하다. DQN 으로 할 때 step 마다 학습하지 않고 데이터만 쌓는다. 나중에 학습과 타겟 신경망 업데이트 하는것이 최적\n",
    "\n",
    "\n",
    "\n",
    "* state : 카트 위치, 카트 속력, 막대 각도, 막대 각속도 - continuous\n",
    "* action : 카트를 오른쪽/왼쪽으로 push\n",
    "* reward : 매 프레임마다 +1, episode 가 끝날때 임의로 보상제공 가능\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "환경 생성\n",
    "'''\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "size_s = env.observation_space.shape[0]\n",
    "size_out = env.action_space.n\n",
    "gamma = .95\n",
    "print(size_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "학습 네트워크 -> target 네트워크 로 복사하는 함수\n",
    "''' \n",
    "def copy(*, dest_scope_name = 'target', src_scope_name = 'main') :\n",
    "    op_holder = []\n",
    "    \n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
    "    \n",
    "    for src_var, dest_var in zip(src_vars, dest_vars) :\n",
    "        op_holder.append(dest_var.assign(src_var))\n",
    "        \n",
    "    return op_holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "replay experience, 공급된 batch 로 학습\n",
    "'''\n",
    "def replay_train(mainDQN, targetDQN, train_batch) :\n",
    "    x_stack = np.empty(0).reshape(0, size_s)\n",
    "    y_stack = np.empty(0).reshape(0, size_out)\n",
    "    \n",
    "    for state, action, reward, next_state, done in train_batch :\n",
    "        \n",
    "        # 현재 Q 는 main 네트워크에서 도출\n",
    "        Q = mainDQN.predict(state)\n",
    "        \n",
    "        # terminal step 이면 음의 보상\n",
    "        if done :\n",
    "            Q[0, action] = -100\n",
    "        \n",
    "        # target Q 는 target 네트워크에서 도출 - 자기상관도 감소\n",
    "        else :\n",
    "            # 그냥 DQN 업데이트 - 타겟 Q 값은 해당 state 에서 max 값을 취한다.\n",
    "            #Q[0, action] = reward + gamma * np.max(targetDQN.predict(next_state))\n",
    "            \n",
    "            # Double DQN 업데이트 - 타겟 Q 값은 main network 로부터 도출한다.\n",
    "            a_target = np.argmax(mainDQN.predict(next_state))\n",
    "            Q[0, action] = reward + gamma * targetDQN.predict(next_state)[0][a_target]\n",
    "            \n",
    "        \n",
    "        # x 는 현재 Q\n",
    "        # y 는 target\n",
    "        x_stack = np.vstack([x_stack, state])\n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        \n",
    "    return mainDQN.update(x_stack, y_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 0, steps : 47\n",
      "episode : 1, steps : 18\n",
      "loss : 433.79947\n",
      "episode : 2, steps : 11\n",
      "episode : 3, steps : 16\n",
      "episode : 4, steps : 91\n",
      "episode : 5, steps : 54\n",
      "episode : 6, steps : 82\n",
      "episode : 7, steps : 66\n",
      "episode : 8, steps : 27\n",
      "episode : 9, steps : 43\n",
      "episode : 10, steps : 57\n",
      "episode : 11, steps : 56\n",
      "loss : 1.2557598\n",
      "episode : 12, steps : 41\n",
      "episode : 13, steps : 52\n",
      "episode : 14, steps : 85\n",
      "episode : 15, steps : 41\n",
      "episode : 16, steps : 16\n",
      "episode : 17, steps : 78\n",
      "episode : 18, steps : 45\n",
      "episode : 19, steps : 22\n",
      "episode : 20, steps : 112\n",
      "episode : 21, steps : 40\n",
      "loss : 1.8581197\n",
      "episode : 22, steps : 32\n",
      "episode : 23, steps : 42\n",
      "episode : 24, steps : 22\n",
      "episode : 25, steps : 82\n",
      "episode : 26, steps : 36\n",
      "episode : 27, steps : 26\n",
      "episode : 28, steps : 23\n",
      "episode : 29, steps : 22\n",
      "episode : 30, steps : 28\n",
      "episode : 31, steps : 17\n",
      "loss : 0.8545225\n",
      "episode : 32, steps : 55\n",
      "episode : 33, steps : 36\n",
      "episode : 34, steps : 47\n",
      "episode : 35, steps : 71\n",
      "episode : 36, steps : 50\n",
      "episode : 37, steps : 59\n",
      "episode : 38, steps : 46\n",
      "episode : 39, steps : 45\n",
      "episode : 40, steps : 50\n",
      "episode : 41, steps : 61\n",
      "loss : 2.9291193\n",
      "episode : 42, steps : 14\n",
      "episode : 43, steps : 11\n",
      "episode : 44, steps : 9\n",
      "episode : 45, steps : 10\n",
      "episode : 46, steps : 9\n",
      "episode : 47, steps : 9\n",
      "episode : 48, steps : 10\n",
      "episode : 49, steps : 11\n",
      "episode : 50, steps : 10\n",
      "episode : 51, steps : 12\n",
      "loss : 2.1472144\n",
      "episode : 52, steps : 39\n",
      "episode : 53, steps : 101\n",
      "episode : 54, steps : 48\n",
      "episode : 55, steps : 65\n",
      "episode : 56, steps : 36\n",
      "episode : 57, steps : 37\n",
      "episode : 58, steps : 29\n",
      "episode : 59, steps : 83\n",
      "episode : 60, steps : 39\n",
      "episode : 61, steps : 40\n",
      "loss : 1.87194\n",
      "episode : 62, steps : 21\n",
      "episode : 63, steps : 41\n",
      "episode : 64, steps : 26\n",
      "episode : 65, steps : 46\n",
      "episode : 66, steps : 39\n",
      "episode : 67, steps : 23\n",
      "episode : 68, steps : 31\n",
      "episode : 69, steps : 41\n",
      "episode : 70, steps : 25\n",
      "episode : 71, steps : 22\n",
      "loss : 4.8436155\n",
      "episode : 72, steps : 22\n",
      "episode : 73, steps : 36\n",
      "episode : 74, steps : 34\n",
      "episode : 75, steps : 24\n",
      "episode : 76, steps : 24\n",
      "episode : 77, steps : 30\n",
      "episode : 78, steps : 18\n",
      "episode : 79, steps : 22\n",
      "episode : 80, steps : 29\n",
      "episode : 81, steps : 36\n",
      "loss : 1048.5612\n",
      "episode : 82, steps : 20\n",
      "episode : 83, steps : 32\n",
      "episode : 84, steps : 23\n",
      "episode : 85, steps : 23\n",
      "episode : 86, steps : 23\n",
      "episode : 87, steps : 24\n",
      "episode : 88, steps : 51\n",
      "episode : 89, steps : 32\n",
      "episode : 90, steps : 39\n",
      "episode : 91, steps : 23\n",
      "loss : 499.6359\n",
      "episode : 92, steps : 10\n",
      "episode : 93, steps : 10\n",
      "episode : 94, steps : 11\n",
      "episode : 95, steps : 9\n",
      "episode : 96, steps : 10\n",
      "episode : 97, steps : 10\n",
      "episode : 98, steps : 8\n",
      "episode : 99, steps : 10\n",
      "episode : 100, steps : 8\n",
      "episode : 101, steps : 13\n",
      "loss : 550.69104\n",
      "episode : 102, steps : 10\n",
      "episode : 103, steps : 10\n",
      "episode : 104, steps : 8\n",
      "episode : 105, steps : 10\n",
      "episode : 106, steps : 10\n",
      "episode : 107, steps : 11\n",
      "episode : 108, steps : 9\n",
      "episode : 109, steps : 10\n",
      "episode : 110, steps : 9\n",
      "episode : 111, steps : 12\n",
      "loss : 477.66568\n",
      "episode : 112, steps : 44\n",
      "episode : 113, steps : 33\n",
      "episode : 114, steps : 26\n",
      "episode : 115, steps : 22\n",
      "episode : 116, steps : 29\n",
      "episode : 117, steps : 25\n",
      "episode : 118, steps : 20\n",
      "episode : 119, steps : 24\n",
      "episode : 120, steps : 25\n",
      "episode : 121, steps : 30\n",
      "loss : 0.6307572\n",
      "episode : 122, steps : 11\n",
      "episode : 123, steps : 10\n",
      "episode : 124, steps : 12\n",
      "episode : 125, steps : 10\n",
      "episode : 126, steps : 9\n",
      "episode : 127, steps : 12\n",
      "episode : 128, steps : 9\n",
      "episode : 129, steps : 8\n",
      "episode : 130, steps : 10\n",
      "episode : 131, steps : 14\n",
      "loss : 507.9912\n",
      "episode : 132, steps : 130\n",
      "episode : 133, steps : 29\n",
      "episode : 134, steps : 81\n",
      "episode : 135, steps : 43\n",
      "episode : 136, steps : 60\n",
      "episode : 137, steps : 103\n",
      "episode : 138, steps : 28\n",
      "episode : 139, steps : 33\n",
      "episode : 140, steps : 116\n",
      "episode : 141, steps : 34\n",
      "loss : 0.7322915\n",
      "episode : 142, steps : 36\n",
      "episode : 143, steps : 66\n",
      "episode : 144, steps : 38\n",
      "episode : 145, steps : 52\n",
      "episode : 146, steps : 39\n",
      "episode : 147, steps : 59\n",
      "episode : 148, steps : 98\n",
      "episode : 149, steps : 61\n",
      "episode : 150, steps : 35\n",
      "episode : 151, steps : 45\n",
      "loss : 2.2231545\n",
      "episode : 152, steps : 110\n",
      "episode : 153, steps : 126\n",
      "episode : 154, steps : 47\n",
      "episode : 155, steps : 58\n",
      "episode : 156, steps : 40\n",
      "episode : 157, steps : 115\n",
      "episode : 158, steps : 77\n",
      "episode : 159, steps : 42\n",
      "episode : 160, steps : 61\n",
      "episode : 161, steps : 90\n",
      "loss : 1.3341535\n",
      "episode : 162, steps : 47\n",
      "episode : 163, steps : 117\n",
      "episode : 164, steps : 289\n",
      "episode : 165, steps : 74\n",
      "episode : 166, steps : 34\n",
      "episode : 167, steps : 55\n",
      "episode : 168, steps : 226\n",
      "episode : 169, steps : 95\n",
      "episode : 170, steps : 40\n",
      "episode : 171, steps : 284\n",
      "loss : 0.47287112\n",
      "episode : 172, steps : 35\n",
      "episode : 173, steps : 35\n",
      "episode : 174, steps : 72\n",
      "episode : 175, steps : 48\n",
      "episode : 176, steps : 73\n",
      "episode : 177, steps : 32\n",
      "episode : 178, steps : 35\n",
      "episode : 179, steps : 31\n",
      "episode : 180, steps : 57\n",
      "episode : 181, steps : 116\n",
      "loss : 1.0935091\n",
      "episode : 182, steps : 37\n",
      "episode : 183, steps : 176\n",
      "episode : 184, steps : 36\n",
      "episode : 185, steps : 82\n",
      "episode : 186, steps : 42\n",
      "episode : 187, steps : 56\n",
      "episode : 188, steps : 32\n",
      "episode : 189, steps : 45\n",
      "episode : 190, steps : 38\n",
      "episode : 191, steps : 23\n",
      "loss : 0.4543139\n",
      "episode : 192, steps : 80\n",
      "episode : 193, steps : 44\n",
      "episode : 194, steps : 48\n",
      "episode : 195, steps : 56\n",
      "episode : 196, steps : 210\n",
      "episode : 197, steps : 120\n",
      "episode : 198, steps : 92\n",
      "episode : 199, steps : 115\n",
      "episode : 200, steps : 92\n",
      "episode : 201, steps : 245\n",
      "loss : 501.67334\n",
      "episode : 202, steps : 74\n",
      "episode : 203, steps : 180\n",
      "episode : 204, steps : 67\n",
      "episode : 205, steps : 109\n",
      "episode : 206, steps : 101\n",
      "episode : 207, steps : 54\n",
      "episode : 208, steps : 51\n",
      "episode : 209, steps : 62\n",
      "episode : 210, steps : 53\n",
      "episode : 211, steps : 54\n",
      "loss : 2.2062497\n",
      "episode : 212, steps : 10\n",
      "episode : 213, steps : 12\n",
      "episode : 214, steps : 10\n",
      "episode : 215, steps : 9\n",
      "episode : 216, steps : 9\n",
      "episode : 217, steps : 8\n",
      "episode : 218, steps : 10\n",
      "episode : 219, steps : 9\n",
      "episode : 220, steps : 9\n",
      "episode : 221, steps : 9\n",
      "loss : 0.9188412\n",
      "episode : 222, steps : 147\n",
      "episode : 223, steps : 42\n",
      "episode : 224, steps : 45\n",
      "episode : 225, steps : 48\n",
      "episode : 226, steps : 106\n",
      "episode : 227, steps : 64\n",
      "episode : 228, steps : 115\n",
      "episode : 229, steps : 46\n",
      "episode : 230, steps : 84\n",
      "episode : 231, steps : 83\n",
      "loss : 0.8261054\n",
      "episode : 232, steps : 9\n",
      "episode : 233, steps : 8\n",
      "episode : 234, steps : 10\n",
      "episode : 235, steps : 10\n",
      "episode : 236, steps : 12\n",
      "episode : 237, steps : 10\n",
      "episode : 238, steps : 12\n",
      "episode : 239, steps : 8\n",
      "episode : 240, steps : 8\n",
      "episode : 241, steps : 10\n",
      "loss : 0.54292285\n",
      "episode : 242, steps : 158\n",
      "episode : 243, steps : 60\n",
      "episode : 244, steps : 51\n",
      "episode : 245, steps : 39\n",
      "episode : 246, steps : 108\n",
      "episode : 247, steps : 43\n",
      "episode : 248, steps : 73\n",
      "episode : 249, steps : 48\n",
      "episode : 250, steps : 218\n",
      "episode : 251, steps : 146\n",
      "loss : 3.211811\n",
      "episode : 252, steps : 73\n",
      "episode : 253, steps : 75\n",
      "episode : 254, steps : 149\n",
      "episode : 255, steps : 260\n",
      "episode : 256, steps : 383\n",
      "episode : 257, steps : 341\n",
      "episode : 258, steps : 205\n",
      "episode : 259, steps : 222\n",
      "episode : 260, steps : 248\n",
      "episode : 261, steps : 76\n",
      "loss : 0.5081178\n",
      "episode : 262, steps : 309\n",
      "episode : 263, steps : 251\n",
      "episode : 264, steps : 144\n",
      "episode : 265, steps : 97\n",
      "episode : 266, steps : 83\n",
      "episode : 267, steps : 213\n",
      "episode : 268, steps : 793\n",
      "episode : 269, steps : 196\n",
      "episode : 270, steps : 250\n",
      "episode : 271, steps : 248\n",
      "loss : 0.44252023\n",
      "episode : 272, steps : 75\n",
      "episode : 273, steps : 248\n",
      "episode : 274, steps : 198\n",
      "episode : 275, steps : 59\n",
      "episode : 276, steps : 255\n",
      "episode : 277, steps : 81\n",
      "episode : 278, steps : 181\n",
      "episode : 279, steps : 71\n",
      "episode : 280, steps : 65\n",
      "episode : 281, steps : 252\n",
      "loss : 3.2740378\n",
      "episode : 282, steps : 9\n",
      "episode : 283, steps : 8\n",
      "episode : 284, steps : 10\n",
      "episode : 285, steps : 9\n",
      "episode : 286, steps : 9\n",
      "episode : 287, steps : 9\n",
      "episode : 288, steps : 9\n",
      "episode : 289, steps : 9\n",
      "episode : 290, steps : 9\n",
      "episode : 291, steps : 9\n",
      "loss : 1.9251773\n",
      "episode : 292, steps : 9\n",
      "episode : 293, steps : 9\n",
      "episode : 294, steps : 10\n",
      "episode : 295, steps : 9\n",
      "episode : 296, steps : 10\n",
      "episode : 297, steps : 8\n",
      "episode : 298, steps : 8\n",
      "episode : 299, steps : 9\n",
      "episode : 300, steps : 10\n",
      "episode : 301, steps : 8\n",
      "loss : 0.82281905\n",
      "episode : 302, steps : 9\n",
      "episode : 303, steps : 12\n",
      "episode : 304, steps : 9\n",
      "episode : 305, steps : 10\n",
      "episode : 306, steps : 8\n",
      "episode : 307, steps : 9\n",
      "episode : 308, steps : 9\n",
      "episode : 309, steps : 8\n",
      "episode : 310, steps : 9\n",
      "episode : 311, steps : 8\n",
      "loss : 520.13605\n",
      "episode : 312, steps : 9\n",
      "episode : 313, steps : 10\n",
      "episode : 314, steps : 9\n",
      "episode : 315, steps : 9\n",
      "episode : 316, steps : 10\n",
      "episode : 317, steps : 9\n",
      "episode : 318, steps : 9\n",
      "episode : 319, steps : 9\n",
      "episode : 320, steps : 9\n",
      "episode : 321, steps : 9\n",
      "loss : 497.17642\n",
      "episode : 322, steps : 206\n",
      "episode : 323, steps : 1001\n",
      "episode : 324, steps : 239\n",
      "episode : 325, steps : 268\n",
      "episode : 326, steps : 416\n",
      "episode : 327, steps : 183\n",
      "episode : 328, steps : 327\n",
      "episode : 329, steps : 173\n",
      "episode : 330, steps : 237\n",
      "episode : 331, steps : 196\n",
      "loss : 1.8294265\n",
      "episode : 332, steps : 9\n",
      "episode : 333, steps : 9\n",
      "episode : 334, steps : 10\n",
      "episode : 335, steps : 9\n",
      "episode : 336, steps : 9\n",
      "episode : 337, steps : 8\n",
      "episode : 338, steps : 9\n",
      "episode : 339, steps : 9\n",
      "episode : 340, steps : 8\n",
      "episode : 341, steps : 9\n",
      "loss : 1.3951426\n",
      "episode : 342, steps : 72\n",
      "episode : 343, steps : 52\n",
      "episode : 344, steps : 85\n",
      "episode : 345, steps : 51\n",
      "episode : 346, steps : 28\n",
      "episode : 347, steps : 33\n",
      "episode : 348, steps : 38\n",
      "episode : 349, steps : 83\n",
      "episode : 350, steps : 34\n",
      "episode : 351, steps : 28\n",
      "loss : 4.4155016\n",
      "episode : 352, steps : 969\n",
      "episode : 353, steps : 393\n",
      "episode : 354, steps : 258\n",
      "episode : 355, steps : 1001\n",
      "episode : 356, steps : 1001\n",
      "episode : 357, steps : 357\n",
      "episode : 358, steps : 176\n",
      "episode : 359, steps : 426\n",
      "episode : 360, steps : 193\n",
      "episode : 361, steps : 449\n",
      "loss : 485.41437\n",
      "episode : 362, steps : 479\n",
      "episode : 363, steps : 592\n",
      "episode : 364, steps : 722\n",
      "episode : 365, steps : 184\n",
      "episode : 366, steps : 277\n",
      "episode : 367, steps : 630\n",
      "episode : 368, steps : 355\n",
      "episode : 369, steps : 289\n",
      "episode : 370, steps : 286\n",
      "episode : 371, steps : 412\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-387417fa793d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[0mminibacth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetDQN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibacth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss :'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c224ef7fa066>\u001b[0m in \u001b[0;36mreplay_train\u001b[1;34m(mainDQN, targetDQN, train_batch)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;31m# Double DQN 업데이트 - 타겟 Q 값은 main network 로부터 도출한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0ma_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtargetDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma_target\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\RL\\dqn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Qpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_X\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# 들어온 데이터를 바탕으로 W 업데이트 (학습 시킨다)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_episode = 1000\n",
    "replay_buffer = deque()\n",
    "ep = 0\n",
    "step = 0\n",
    "#saver = tf.train.Saver()\n",
    "#save_file = 'C:\\\\Users\\\\김민수\\\\Documents\\\\GitHub\\\\RL\\\\vars\\\\cartpole_dqn'\n",
    "mean_reward = deque()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "      \n",
    "    '''\n",
    "    - 네트워크 2 개 생성\n",
    "    - 하나는 실제 학습을 하는 학습 네트워크\n",
    "    - 다른 하나는 일시적으로 고정되어 학습의 목표가 되는 target 네트워크\n",
    "    '''\n",
    "    mainDQN = dqn.DQN(sess, size_s, size_out, 8, 6, name='main')\n",
    "    targetDQN = dqn.DQN(sess, size_s, size_out, 8, 6, name='target')\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # 처음 시작 시 네트워크 복사\n",
    "    copy_ops = copy(dest_scope_name='targetDQN', src_scope_name='mainDQN')\n",
    "    sess.run(copy_ops)\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for episode in range(total_episode) :\n",
    "        e = 1. / ((episode / 20) + 1)    # 점점 감소하는 explore \n",
    "        done = False\n",
    "        step_count = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not done :\n",
    "             \n",
    "            # 액션 선택\n",
    "            if np.random.rand(1) < e :\n",
    "                action = env.action_space.sample()\n",
    "            else :\n",
    "                action = np.argmax(mainDQN.predict(state))\n",
    "                    \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "            if done : \n",
    "                reward = -100\n",
    "                \n",
    "                \n",
    "            # 학습하지않고 임시메모리에 저장한다.\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if(len(replay_buffer) > 50000) :\n",
    "                replay_buffer.popleft()\n",
    "            \n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            \n",
    "            if(step_count > 1000) :\n",
    "                break\n",
    "        \n",
    "        \n",
    "        print ('episode : {}, steps : {}'.format(episode, step_count))\n",
    "                \n",
    "        '''\n",
    "        파라미터 로컬에 저장.\n",
    "        '''\n",
    "        if(len(mean_reward) < 10):\n",
    "            mean_reward.append(step_count)\n",
    "        else:\n",
    "            mean_reward.popleft()\n",
    "            \n",
    "            '''\n",
    "            if(np.mean(mean_reward) > 800):\n",
    "                saver.save(sess, save_file)\n",
    "                break\n",
    "            '''\n",
    "                \n",
    "        # 10 회의 에피소드가 끝나면 임시메모리에서 과거 데이터를 랜덤으로 뽑아 학습한다.\n",
    "        if(episode % 10 == 1) :\n",
    "            for _ in range(50) :\n",
    "                minibacth = random.sample(replay_buffer, 10)\n",
    "                loss, _ = replay_train(mainDQN, targetDQN, minibacth)\n",
    "                        \n",
    "            print('loss :', loss)\n",
    "            sess.run(copy_ops)   # 목표가 되는 네트워크를 업데이트한다.\n",
    "                    \n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
