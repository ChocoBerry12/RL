{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole\n",
    "---\n",
    ">continous 한 action 환경 중에서 가장 단순한 환경이다. 매 프레임마다 reward 를 제공하는 dense reward 환경이라서 때문에 학습이 매우 용이하다. \n",
    "또한 terminal condition 은 카트가 일정 범위 넘어가거나 막대가 일정 각도 이상되는 것이기 때문에 명확하다. DQN 으로 할 때 step 마다 학습하지 않고 데이터만 쌓는다. 나중에 학습과 타겟 신경망 업데이트 하는것이 최적\n",
    "\n",
    "\n",
    "\n",
    "* state : 카트 위치, 카트 속력, 막대 각도, 막대 각속도 - continuous\n",
    "* action : 카트를 오른쪽/왼쪽으로 push\n",
    "* reward : 매 프레임마다 +1, episode 가 끝날때 임의로 보상제공 가능\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "환경 생성\n",
    "'''\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "size_s = env.observation_space.shape[0]\n",
    "size_out = env.action_space.n\n",
    "gamma = .95\n",
    "print(size_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "학습 네트워크 -> target 네트워크 로 복사하는 함수\n",
    "''' \n",
    "def copy(*, dest_scope_name = 'target', src_scope_name = 'main') :\n",
    "    op_holder = []\n",
    "    \n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
    "    \n",
    "    for src_var, dest_var in zip(src_vars, dest_vars) :\n",
    "        op_holder.append(dest_var.assign(src_var))\n",
    "        \n",
    "    return op_holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "replay experience, 공급된 batch 로 학습\n",
    "'''\n",
    "def replay_train(mainDQN, targetDQN, train_batch) :\n",
    "    x_stack = np.empty(0).reshape(0, size_s)\n",
    "    y_stack = np.empty(0).reshape(0, size_out)\n",
    "    \n",
    "    for state, action, reward, next_state, done in train_batch :\n",
    "        \n",
    "        # 현재 Q 는 main 네트워크에서 도출\n",
    "        Q = mainDQN.predict(state)\n",
    "        \n",
    "        # terminal step 이면 음의 보상\n",
    "        if done :\n",
    "            Q[0, action] = -100\n",
    "        \n",
    "        # target Q 는 target 네트워크에서 도출 - 자기상관도 감소\n",
    "        else :\n",
    "            # 그냥 DQN 업데이트 - 타겟 Q 값은 해당 state 에서 max 값을 취한다.\n",
    "            #Q[0, action] = reward + gamma * np.max(targetDQN.predict(next_state))\n",
    "            \n",
    "            # Double DQN 업데이트 - 타겟 Q 값은 main network 로부터 도출한다.\n",
    "            a_target = np.argmax(mainDQN.predict(next_state))\n",
    "            Q[0, action] = reward + gamma * targetDQN.predict(next_state)[0][a_target]\n",
    "            \n",
    "        \n",
    "        # x 는 현재 Q\n",
    "        # y 는 target\n",
    "        x_stack = np.vstack([x_stack, state])\n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        \n",
    "    return mainDQN.update(x_stack, y_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 0, steps : 10\n",
      "episode : 1, steps : 24\n",
      "loss : 489.00653\n",
      "episode : 2, steps : 34\n",
      "episode : 3, steps : 28\n",
      "episode : 4, steps : 15\n",
      "episode : 5, steps : 13\n",
      "episode : 6, steps : 11\n",
      "episode : 7, steps : 10\n",
      "episode : 8, steps : 15\n",
      "episode : 9, steps : 14\n",
      "episode : 10, steps : 14\n",
      "episode : 11, steps : 15\n",
      "loss : 2.6621509\n",
      "episode : 12, steps : 39\n",
      "episode : 13, steps : 39\n",
      "episode : 14, steps : 51\n",
      "episode : 15, steps : 56\n",
      "episode : 16, steps : 60\n",
      "episode : 17, steps : 35\n",
      "episode : 18, steps : 152\n",
      "episode : 19, steps : 20\n",
      "episode : 20, steps : 39\n",
      "episode : 21, steps : 31\n",
      "loss : 1.9861908\n",
      "episode : 22, steps : 12\n",
      "episode : 23, steps : 9\n",
      "episode : 24, steps : 8\n",
      "episode : 25, steps : 12\n",
      "episode : 26, steps : 9\n",
      "episode : 27, steps : 13\n",
      "episode : 28, steps : 9\n",
      "episode : 29, steps : 10\n",
      "episode : 30, steps : 13\n",
      "episode : 31, steps : 12\n",
      "loss : 4.4117866\n",
      "episode : 32, steps : 42\n",
      "episode : 33, steps : 82\n",
      "episode : 34, steps : 42\n",
      "episode : 35, steps : 61\n",
      "episode : 36, steps : 60\n",
      "episode : 37, steps : 69\n",
      "episode : 38, steps : 51\n",
      "episode : 39, steps : 100\n",
      "episode : 40, steps : 124\n",
      "episode : 41, steps : 34\n",
      "loss : 526.51025\n",
      "episode : 42, steps : 11\n",
      "episode : 43, steps : 11\n",
      "episode : 44, steps : 10\n",
      "episode : 45, steps : 9\n",
      "episode : 46, steps : 12\n",
      "episode : 47, steps : 13\n",
      "episode : 48, steps : 10\n",
      "episode : 49, steps : 12\n",
      "episode : 50, steps : 10\n",
      "episode : 51, steps : 17\n",
      "loss : 1.4987669\n",
      "episode : 52, steps : 34\n",
      "episode : 53, steps : 49\n",
      "episode : 54, steps : 32\n",
      "episode : 55, steps : 45\n",
      "episode : 56, steps : 31\n",
      "episode : 57, steps : 83\n",
      "episode : 58, steps : 23\n",
      "episode : 59, steps : 47\n",
      "episode : 60, steps : 33\n",
      "episode : 61, steps : 45\n",
      "loss : 2.5791724\n",
      "episode : 62, steps : 11\n",
      "episode : 63, steps : 10\n",
      "episode : 64, steps : 10\n",
      "episode : 65, steps : 11\n",
      "episode : 66, steps : 12\n",
      "episode : 67, steps : 11\n",
      "episode : 68, steps : 8\n",
      "episode : 69, steps : 10\n",
      "episode : 70, steps : 10\n",
      "episode : 71, steps : 11\n",
      "loss : 0.58635813\n",
      "episode : 72, steps : 10\n",
      "episode : 73, steps : 53\n",
      "episode : 74, steps : 15\n",
      "episode : 75, steps : 63\n",
      "episode : 76, steps : 28\n",
      "episode : 77, steps : 34\n",
      "episode : 78, steps : 13\n",
      "episode : 79, steps : 41\n",
      "episode : 80, steps : 43\n",
      "episode : 81, steps : 17\n",
      "loss : 0.57743394\n",
      "episode : 82, steps : 357\n",
      "episode : 83, steps : 439\n",
      "episode : 84, steps : 389\n",
      "episode : 85, steps : 626\n",
      "episode : 86, steps : 275\n",
      "episode : 87, steps : 257\n",
      "episode : 88, steps : 715\n",
      "episode : 89, steps : 349\n",
      "episode : 90, steps : 683\n",
      "episode : 91, steps : 539\n",
      "loss : 0.7354508\n",
      "episode : 92, steps : 193\n",
      "episode : 93, steps : 113\n",
      "episode : 94, steps : 148\n",
      "episode : 95, steps : 73\n",
      "episode : 96, steps : 191\n",
      "episode : 97, steps : 206\n",
      "episode : 98, steps : 224\n",
      "episode : 99, steps : 209\n",
      "episode : 100, steps : 127\n",
      "episode : 101, steps : 178\n",
      "loss : 0.83702934\n",
      "episode : 102, steps : 9\n",
      "episode : 103, steps : 9\n",
      "episode : 104, steps : 10\n",
      "episode : 105, steps : 10\n",
      "episode : 106, steps : 10\n",
      "episode : 107, steps : 9\n",
      "episode : 108, steps : 10\n",
      "episode : 109, steps : 8\n",
      "episode : 110, steps : 9\n",
      "episode : 111, steps : 10\n",
      "loss : 0.99163675\n",
      "episode : 112, steps : 220\n",
      "episode : 113, steps : 299\n",
      "episode : 114, steps : 256\n",
      "episode : 115, steps : 204\n",
      "episode : 116, steps : 175\n",
      "episode : 117, steps : 279\n",
      "episode : 118, steps : 280\n",
      "episode : 119, steps : 208\n",
      "episode : 120, steps : 226\n",
      "episode : 121, steps : 370\n",
      "loss : 0.3919418\n",
      "episode : 122, steps : 240\n",
      "episode : 123, steps : 424\n",
      "episode : 124, steps : 192\n",
      "episode : 125, steps : 332\n",
      "episode : 126, steps : 253\n",
      "episode : 127, steps : 282\n",
      "episode : 128, steps : 183\n",
      "episode : 129, steps : 141\n",
      "episode : 130, steps : 303\n",
      "episode : 131, steps : 178\n",
      "loss : 0.61373824\n",
      "episode : 132, steps : 14\n",
      "episode : 133, steps : 10\n",
      "episode : 134, steps : 9\n",
      "episode : 135, steps : 18\n",
      "episode : 136, steps : 38\n",
      "episode : 137, steps : 14\n",
      "episode : 138, steps : 25\n",
      "episode : 139, steps : 13\n",
      "episode : 140, steps : 12\n",
      "episode : 141, steps : 11\n",
      "loss : 0.4253624\n",
      "episode : 142, steps : 679\n",
      "episode : 143, steps : 296\n",
      "episode : 144, steps : 327\n",
      "episode : 145, steps : 296\n",
      "episode : 146, steps : 668\n",
      "episode : 147, steps : 670\n",
      "episode : 148, steps : 383\n",
      "episode : 149, steps : 476\n",
      "episode : 150, steps : 654\n",
      "episode : 151, steps : 579\n",
      "loss : 0.29361498\n",
      "episode : 152, steps : 266\n",
      "episode : 153, steps : 160\n",
      "episode : 154, steps : 181\n",
      "episode : 155, steps : 179\n",
      "episode : 156, steps : 455\n",
      "episode : 157, steps : 579\n",
      "episode : 158, steps : 364\n",
      "episode : 159, steps : 609\n",
      "episode : 160, steps : 667\n",
      "episode : 161, steps : 239\n",
      "loss : 0.34004158\n",
      "episode : 162, steps : 877\n",
      "episode : 163, steps : 1001\n",
      "episode : 164, steps : 418\n",
      "episode : 165, steps : 274\n",
      "episode : 166, steps : 434\n",
      "episode : 167, steps : 809\n",
      "episode : 168, steps : 442\n",
      "episode : 169, steps : 316\n",
      "episode : 170, steps : 418\n",
      "episode : 171, steps : 942\n",
      "loss : 0.4690996\n",
      "episode : 172, steps : 334\n",
      "episode : 173, steps : 337\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-387417fa793d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\RL\\dqn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Qpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_X\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# 들어온 데이터를 바탕으로 W 업데이트 (학습 시킨다)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_episode = 1000\n",
    "replay_buffer = deque()\n",
    "ep = 0\n",
    "step = 0\n",
    "#saver = tf.train.Saver()\n",
    "#save_file = 'C:\\\\Users\\\\김민수\\\\Documents\\\\GitHub\\\\RL\\\\vars\\\\cartpole_dqn'\n",
    "mean_reward = deque()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "      \n",
    "    '''\n",
    "    - 네트워크 2 개 생성\n",
    "    - 하나는 실제 학습을 하는 학습 네트워크\n",
    "    - 다른 하나는 일시적으로 고정되어 학습의 목표가 되는 target 네트워크\n",
    "    '''\n",
    "    mainDQN = dqn.DQN(sess, size_s, size_out, 8, 6, name='main')\n",
    "    targetDQN = dqn.DQN(sess, size_s, size_out, 8, 6, name='target')\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # 처음 시작 시 네트워크 복사\n",
    "    copy_ops = copy(dest_scope_name='targetDQN', src_scope_name='mainDQN')\n",
    "    sess.run(copy_ops)\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for episode in range(total_episode) :\n",
    "        e = 1. / ((episode / 20) + 1)    # 점점 감소하는 explore \n",
    "        done = False\n",
    "        step_count = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not done :\n",
    "             \n",
    "            # 액션 선택\n",
    "            if np.random.rand(1) < e :\n",
    "                action = env.action_space.sample()\n",
    "            else :\n",
    "                action = np.argmax(mainDQN.predict(state))\n",
    "                    \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "            if done : \n",
    "                reward = -100\n",
    "                \n",
    "                \n",
    "            # 학습하지않고 임시메모리에 저장한다.\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            if(len(replay_buffer) > 50000) :\n",
    "                replay_buffer.popleft()\n",
    "            \n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            \n",
    "            if(step_count > 1000) :\n",
    "                break\n",
    "        \n",
    "        \n",
    "        print ('episode : {}, steps : {}'.format(episode, step_count))\n",
    "                \n",
    "        '''\n",
    "        파라미터 로컬에 저장.\n",
    "        '''\n",
    "        if(len(mean_reward) < 10):\n",
    "            mean_reward.append(step_count)\n",
    "        else:\n",
    "            mean_reward.popleft()\n",
    "            \n",
    "            '''\n",
    "            if(np.mean(mean_reward) > 800):\n",
    "                saver.save(sess, save_file)\n",
    "                break\n",
    "            '''\n",
    "                \n",
    "        # 10 회의 에피소드가 끝나면 임시메모리에서 과거 데이터를 랜덤으로 뽑아 학습한다.\n",
    "        if(episode % 10 == 1) :\n",
    "            for _ in range(50) :\n",
    "                minibacth = random.sample(replay_buffer, 10)\n",
    "                loss, _ = replay_train(mainDQN, targetDQN, minibacth)\n",
    "                        \n",
    "            print('loss :', loss)\n",
    "            sess.run(copy_ops)   # 목표가 되는 네트워크를 업데이트한다.\n",
    "                    \n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
