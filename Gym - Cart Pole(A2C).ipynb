{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C (Advantage Actor Critic)\n",
    "---\n",
    "## 환경\n",
    "* state : discrete[16]\n",
    "* action : discrete[4]\n",
    "* 목표에 도달 시 +1 점, 구멍에 빠질 때 -1 점, 나머지 0 점.\n",
    "\n",
    "## 환경\n",
    "* input : state (array[16])\n",
    "* output : Q (prob)\n",
    "---\n",
    "* A2C 알고리즘은 Critic 을 도입한 Policy Gradient 알고리즘이다\n",
    "* 기본적으로 on-policy 알고리즘이기 때문에 자기상관도에 의해 성능이 떨어진다.\n",
    "* 이는 A3C 나 다른 off-policy 기법(Importance Sampling)을 사용해야 한다.\n",
    "* 혹은 batch 방식이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "환경 생성\n",
    "'''\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'C:\\\\Users\\\\김민수\\\\Documents\\\\GitHub\\\\RL\\\\vars\\\\cartpole_pg'\n",
    "lr = .0001 ## learning rate\n",
    "total_episode = 1000\n",
    "epsilon = 1\n",
    "gamma = .95  ##Discounted factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network\n",
    "size_in = env.observation_space.shape[0]\n",
    "size_out = env.action_space.n\n",
    "size_w1 = 12\n",
    "size_w2 = 18\n",
    "size_w3 = 8\n",
    "\n",
    "Advantage = tf.placeholder(tf.float32)\n",
    "STATE_IN = tf.placeholder(tf.float32, [None, size_in])\n",
    "A_1 = tf.Variable(tf.random_normal([size_in, size_w1],stddev=.01), name='A_1')\n",
    "A_2 = tf.Variable(tf.random_normal([size_w1, size_w2],stddev=.01), name='A_2')\n",
    "A_3 = tf.Variable(tf.random_normal([size_w2, size_w3],stddev=.01), name='A_3')\n",
    "out = tf.Variable(tf.random_normal([size_w3, size_out],stddev=.01), name='out')\n",
    "\n",
    "L_1 = (tf.matmul(STATE_IN, A_1))\n",
    "L_2 = (tf.matmul(L_1, A_2))\n",
    "L_3 = (tf.matmul(L_2, A_3))\n",
    "L_out = tf.sigmoid(tf.matmul(L_3, out))\n",
    "\n",
    "loss = -tf.reduce_mean(tf.log(L_out) * Advantage)\n",
    "train = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "# 출력은 single 실수값 value\n",
    "targetV = tf.placeholder(tf.float32)\n",
    "STATE_IN_c = tf.placeholder(tf.float32, [None, size_in])\n",
    "C_1 = tf.Variable(tf.random_normal([size_in, size_w1],stddev=.01), name='C_1')\n",
    "C_2 = tf.Variable(tf.random_normal([size_w1, size_w2],stddev=.01), name='C_2')\n",
    "C_3 = tf.Variable(tf.random_normal([size_w2, size_w3],stddev=.01), name='C_3')\n",
    "out_c = tf.Variable(tf.random_normal([size_w3, 1],stddev=.01), name='out')\n",
    "\n",
    "Lc_1 = (tf.matmul(STATE_IN_c, C_1))\n",
    "Lc_2 = (tf.matmul(Lc_1, C_2))\n",
    "Lc_3 = (tf.matmul(Lc_2, C_3))\n",
    "Lc_out = tf.sigmoid(tf.matmul(Lc_3, out_c))\n",
    "\n",
    "loss_c = tf.reduce_mean(tf.square(Lc_out - targetV))\n",
    "train_c = tf.train.AdamOptimizer(lr).minimize(loss_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1, score : 33 \n",
      "episode 2, score : 25 \n",
      "episode 3, score : 56 \n",
      "episode 4, score : 11 \n",
      "episode 5, score : 26 \n",
      "episode 6, score : 14 \n",
      "episode 7, score : 15 \n",
      "episode 8, score : 54 \n",
      "episode 9, score : 33 \n",
      "episode 10, score : 29 \n",
      "episode 11, score : 18 \n",
      "episode 12, score : 15 \n",
      "episode 13, score : 20 \n",
      "episode 14, score : 8 \n",
      "episode 15, score : 18 \n",
      "episode 16, score : 19 \n",
      "episode 17, score : 16 \n",
      "episode 18, score : 39 \n",
      "episode 19, score : 20 \n",
      "episode 20, score : 20 \n",
      "episode 21, score : 20 \n",
      "episode 22, score : 10 \n",
      "episode 23, score : 13 \n",
      "episode 24, score : 41 \n",
      "episode 25, score : 27 \n",
      "episode 26, score : 91 \n",
      "episode 27, score : 28 \n",
      "episode 28, score : 36 \n",
      "episode 29, score : 26 \n",
      "episode 30, score : 15 \n",
      "episode 31, score : 49 \n",
      "episode 32, score : 23 "
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    state = env.reset()\n",
    "    ep = 1\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    while ep < total_episode:\n",
    "        step += 1\n",
    "        #env.render()\n",
    "        state = np.reshape(state, newshape=[1, size_in])\n",
    "        Q_prev = sess.run(L_out, feed_dict={STATE_IN:state})\n",
    "        \n",
    "        # actor 로 액션선택\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_prev)\n",
    "            \n",
    "        new_state, r, d, _ = env.step(action)\n",
    "        new_state = np.reshape(new_state, newshape=[1, size_in])\n",
    "        \n",
    "        # value 계산\n",
    "        value = sess.run(Lc_out, feed_dict={STATE_IN_c:state})[0][0]\n",
    "        value_next = sess.run(Lc_out, feed_dict={STATE_IN_c:new_state})[0][0]\n",
    "        \n",
    "        # 도중에 쓰러지면 punish\n",
    "        if d:\n",
    "            advantage = -100\n",
    "            target = -100        # - 보상(벌) 을 주어서 그 액션은 하지 않도록 강화한다.\n",
    "            epsilon = 1 / ((ep/100) + 1)\n",
    "            print(\"episode {}, score : {} \".format(ep, step))\n",
    "            step = 0\n",
    "            ep += 1\n",
    "            state = env.reset()\n",
    "        # advantage 와 targetV 계산\n",
    "        else:\n",
    "            target = r + gamma * value_next\n",
    "            advantage = r + gamma * value_next - value            \n",
    "            state = new_state\n",
    "        \n",
    "        if(np.shape(state) != (1, 4)):\n",
    "            state = np.reshape(state, newshape=[1, size_in])\n",
    "        \n",
    "        # Actor 와 Critic 함께 학습.\n",
    "        sess.run(train, feed_dict={STATE_IN:state, Advantage:advantage})\n",
    "        sess.run(train_c, feed_dict={STATE_IN_c:state, targetV:target})\n",
    "            \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
