{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "![title](https://spinningup.openai.com/en/latest/_images/math/0a399dc49e3b45664a7edaf485ab5c23a7282f43.svg)\n",
    "---\n",
    "## torch 신경망 주의할 것\n",
    "* 업데이트 할 파라미터 정확히 지정하기 - detach 로 학습할 파라미터 확실하게 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 32)\n",
    "        self.fc_pi = nn.Linear(32, 2)\n",
    "        self.fc_v = nn.Linear(32,1)\n",
    "    \n",
    "    def pi(self, x):\n",
    "        x = self.fc1(x)\n",
    "        prob = torch.softmax(self.fc_pi(x), dim = 0)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc_v(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_factory(memory):\n",
    "    s_, a_, r_, s2_, d_, prob_ = [], [], [], [], [], []\n",
    "    for s, a, r, s2, d, p in memory:\n",
    "        s_.append(s)\n",
    "        a_.append([a])\n",
    "        r_.append([r])\n",
    "        s2_.append(s2)\n",
    "        d_.append([d])\n",
    "        prob_.append(p)\n",
    "        \n",
    "    s_ = torch.tensor(s_, dtype=torch.float)\n",
    "    a_ = torch.tensor(a_)\n",
    "    r_ = torch.tensor(r_)\n",
    "    s2_ = torch.tensor(s2_, dtype=torch.float)\n",
    "    d_ = torch.tensor(d_, dtype=torch.float)\n",
    "    prob_ = torch.tensor(prob_)\n",
    "    \n",
    "    return s_, a_, r_, s2_, d_, prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(memory, item):\n",
    "    memory.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(memory, net):\n",
    "    s, a, r, s2, d, prob = batch_factory(memory)\n",
    "    \n",
    "    target = r + gamma * net.v(s2)\n",
    "    delta = net.v(s) - target\n",
    "    delta = delta.detach().numpy()\n",
    "    \n",
    "    advantage_lst = []\n",
    "    advantage = 0.0\n",
    "    for delta_t in delta[::-1]:\n",
    "        advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "        advantage_lst.append([advantage])\n",
    "    advantage_lst.reverse()\n",
    "    advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "    pi = net.pi(s)\n",
    "    pi_a = pi.gather(1, a)\n",
    "    ratio = pi_a / prob\n",
    "    \n",
    "    surr1 = ratio * advantage\n",
    "    surr2 = torch.clamp(ratio, 1 - e, 1 + e) * advantage\n",
    "    loss = -torch.min(surr1, surr2) + F.mse_loss(net.v(s), target.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PPO()\n",
    "ep = 1\n",
    "total_ep = 100\n",
    "gamma = .95\n",
    "e = .1\n",
    "\n",
    "while(ep < total_ep):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    buffer = collections.deque()\n",
    "    while(not done):\n",
    "        prob = net.pi(torch.from_numpy(state).float())\n",
    "        action = Categorical(prob).sample().item()\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        stack(buffer, (state, action, reward, state_next, done, prob[action].item()))\n",
    "        if(done):\n",
    "            train(buffer, net)\n",
    "            ep += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
